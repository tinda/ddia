# 10. 批次處理 

![](img/ch10.png)

> 帶有太強個人色彩的系統無法成功。當最初的設計完成並且相對穩定時，不同的人們以自己的方式進行測試，真正的考驗才開始。
>
> ——高德納

---------------

[TOC]

   在本書的前兩部分中，我們討論了很多關於**請求**和**查詢**以及相應的**回應**或**結果**。許多現有資料系統中都採用這種資料處理方式：你發送請求指令，一段時間後(我們期望)系統會給出一個結果。資料庫，緩存，搜索索引，Web伺服器以及其他一些系統都以這種方式工作。

   像這樣的**線上（online）**系統，無論是流覽器請求頁面還是調用遠端API的服務，我們通常認為請求是由人類用戶觸發的，並且正在等待響應。他們不應該等太久，所以我們非常關注系統的回應時間（參閱“[描述性能](ch1.md)”）。

   Web和越來越多的基於HTTP/REST的API使交互的請求/回應風格變得如此普遍，以至於很容易將其視為理所當然。但我們應該記住，這不是構建系統的唯一方式，其他方法也有其優點。我們來看看三種不同類型的系統：

 ***服務（線上系統）***

   服務等待客戶的請求或指令到達。每收到一個，服務會試圖儘快處理它，併發回一個回應。回應時間通常是服務性能的主要衡量指標，可用性通常非常重要（如果用戶端無法訪問服務，使用者可能會收到錯誤消息）。

***批次處理系統（離線系統）***

   一個批次處理系統有大量的輸入資料，跑一個**作業（job）**來處理它，並生成一些輸出資料，這往往需要一段時間（從幾分鐘到幾天），所以通常不會有用戶等待作業完成。相反，批量作業通常會定期運行（例如，每天一次）。批次處理作業的主要性能衡量標準通常是輸送量（處理特定大小的輸入所需的時間）。本章中討論的就是批次處理。

***流處理系統（准即時系統）***

   流處理介於線上和離線（批次處理）之間，所以有時候被稱為**准即時（near-real-time）**或**准線上（nearline）**處理。像批次處理系統一樣，流處理消費輸入並產生輸出（並不需要回應請求）。但是，流式作業在事件發生後不久就會對事件進行操作，而批次處理作業則需等待固定的一組輸入資料。這種差異使流處理系統比起批次處理系統具有更低的延遲。由於流處理基於批次處理，我們將在[第11章](ch11.md)討論它。

   正如我們將在本章中看到的那樣，批次處理是構建可靠，可擴展和可維護應用程式的重要組成部分。例如，2004年發佈的批次處理演算法Map-Reduce（可能被過分熱情地）被稱為“造就Google大規模可擴展性的演算法”【2】。隨後在各種開來源資料系統中得到應用，包括Hadoop，CouchDB和MongoDB。

   與多年前為資料倉庫開發的並行處理系統【3,4】相比，MapReduce是一個相當低級別的程式設計模型，但它使得在商用硬體上能進行的處理規模邁上一個新的臺階。雖然MapReduce的重要性正在下降【5】，但它仍然值得去理解，因為它描繪了一幅關於批次處理為什麼有用，以及如何實用的清晰圖景。

   實際上，批次處理是一種非常古老的計算方式。早在可程式設計數位電腦誕生之前，打孔卡製表機（例如1890年美國人口普查【6】中使用的霍爾裡斯機）實現了半機械化的批次處理形式，從大量輸入中匯總計算。 Map-Reduce與1940年代和1950年代廣泛用於商業資料處理的機電IBM卡片分類機器有著驚人的相似之處【7】。正如我們所說，歷史總是在不斷重複自己。

   在本章中，我們將瞭解MapReduce和其他一些批次處理演算法和框架，並探索它們在現代資料系統中的作用。但首先我們將看看使用標準Unix工具的資料處理。即使你已經熟悉了它們，Unix的哲學也值得一讀，Unix的思想和經驗教訓可以遷移到大規模，異構的分散式資料系統中。


## 使用Unix工具的批次處理

   我們從一個簡單的例子開始。假設您有一台Web伺服器，每次處理請求時都會在日誌檔中附加一行。例如，使用nginx預設訪問日誌格式，日誌的一行可能如下所示：

```bash
216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1" 
200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) 
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36"
```

（實際上這只是一行，分成多行只是為了便於閱讀。）這一行中有很多資訊。為了解釋它，你需要瞭解日誌格式的定義，如下所示：

```
 $remote_addr - $remote_user [$time_local] "$request"
 $status $body_bytes_sent "$http_referer" "$http_user_agent"
```

   日誌的這一行表明在2015年2月27日17:55:11 UTC，伺服器從用戶端IP位址`216.58.210.78`接收到對文件`/css/typography.css`的請求。用戶沒有被認證，所以`$remote_user`被設置為連字號（`-` ）。回應狀態是200（即請求成功），回應的大小是3377位元組。網頁流覽器是Chrome 40，URL `http://martin.kleppmann.com/` 的頁面中的引用導致該檔被載入。


### 分析簡單日誌

   很多工具可以從這些日誌檔生成關於網站流量的漂亮的報告，但為了練手，讓我們使用基本的Unix功能創建自己的工具。 例如，假設你想在你的網站上找到五個最受歡迎的網頁。 則可以在Unix shell中這樣做：[^i]

[^i]: 有些人認為`cat`這裡並沒有必要，因為輸入檔可以直接作為awk的參數。 但這種寫法讓線性管道更為顯眼。

```bash
cat /var/log/nginx/access.log | #1
    awk '{print $7}' | #2
    sort             | #3
    uniq -c          | #4
    sort -r -n       | #5
    head -n 5          #6
```

1. 讀取日誌檔
2. 將每一行按空格分割成不同的欄位，每行只輸出第七個欄位，恰好是請求的URL。在我們的例子中是`/css/typography.css`。
3. 按字母順序排列請求的URL清單。如果某個URL被請求過n次，那麼排序後，檔將包含連續重複出現n次的該URL。
4. `uniq`命令通過檢查兩個相鄰的行是否相同來過濾掉輸入中的重複行。 `-c`則表示還要輸出一個計數器：對於每個不同的URL，它會報告輸入中出現該URL的次數。
5. 第二種排序按每行起始處的數字（`-n`）排序，這是URL的請求次數。然後逆序（`-r`）返回結果，大的數字在前。
6. 最後，只輸出前五行（`-n 5`），並丟棄其餘的。該系列命令的輸出如下所示：

```
    4189 /favicon.ico
    3631 /2013/05/24/improving-security-of-ssh-private-keys.html
    2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
    1369 /
     915 /css/typography.css
```

   如果你不熟悉Unix工具，上面的命令列可能看起來有點吃力，但是它非常強大。它能在幾秒鐘內處理幾GB的日誌檔，並且您可以根據需要輕鬆修改命令。例如，如果要從報告中省略CSS檔，可以將awk參數更改為`'$7 !~ /\.css$/ {print $7}'`,如果想統計最多的用戶端IP位址,可以把awk參數改為`'{print $1}'`等等。

   我們不會在這裡詳細探索Unix工具，但是它非常值得學習。令人驚訝的是，使用awk，sed，grep，sort，uniq和xargs的組合，可以在幾分鐘內完成許多資料分析，並且它們的性能相當的好【8】。

#### 命令鏈與自訂程式

除了Unix命令鏈，你還可以寫一個簡單的程式來做同樣的事情。例如在Ruby中，它可能看起來像這樣：

```ruby
counts = Hash.new(0)         # 1
File.open('/var/log/nginx/access.log') do |file| 
    file.each do |line|
        url = line.split[6]  # 2
        counts[url] += 1     # 3
    end
end

top5 = counts.map{|url, count| [count, url] }.sort.reverse[0...5] # 4
top5.each{|count, url| puts "#{count} #{url}" }                   # 5
```

1. `counts`是一個存儲計數器的雜湊表，保存了每個URL被流覽的次數，默認為0。
2. 逐行讀取日誌，抽取每行第七個被空格分隔的欄位為URL（這裡的陣列索引是6，因為Ruby的陣列索引從0開始計數）
3. 將日誌當前行中URL對應的計數器值加一。
4. 按計數器值（降冪）對雜湊表內容進行排序，並取前五位。
5. 列印出前五個條目。

這個程式並不像Unix管道那樣簡潔，但是它的可讀性很強，喜歡哪一種屬於口味的問題。但兩者除了表面上的差異之外，執行流程也有很大差異，如果你在大檔上運行此分析，則會變得明顯。

#### 排序 VS 記憶體中的聚合

   Ruby腳本在記憶體中保存了一個URL的雜湊表，將每個URL映射到它出現的次數。 Unix管道沒有這樣的雜湊表，而是依賴於對URL列表的排序，在這個URL列表中，同一個URL的只是簡單地重複出現。

   哪種方法更好？這取決於你有多少個不同的URL。對於大多數中小型網站，你可能可以為所有不同網址提供一個計數器（假設我們使用1GB記憶體）。在此例中，作業的**工作集（working set）**（作業需要隨機訪問的記憶體大小）僅取決於不同URL的數量：如果日誌中只有單個URL，重複出現一百萬次，則散列表所需的空間表就只有一個URL加上一個計數器的大小。當工作集足夠小時，記憶體散清單表現良好，甚至在性能較差的筆記型電腦上也可以正常工作。

   另一方面，如果作業的工作集大於可用記憶體，則排序方法的優點是可以高效地使用磁片。這與我們在“[SSTables和LSM樹](ch3.md#SSTables和LSM樹)”中討論過的原理是一樣的：資料塊可以在記憶體中排序並作為段檔寫入磁片，然後多個排序好的段可以合併為一個更大的排序檔。 歸併排序具有在磁片上運行良好的循序存取模式。 （請記住，針對順序I/O進行優化是[第3章](ch3.md)中反復出現的主題，相同的模式在此重現）

   GNU Coreutils（Linux）中的`sort `程式通過溢出至磁片的方式來自動應對大於記憶體的資料集，並能同時使用多個CPU核進行並行排序【9】。這意味著我們之前看到的簡單的Unix命令鏈很容易擴展到大資料集，且不會耗盡記憶體。瓶頸可能是從磁片讀取輸入檔的速度。


### Unix哲學

   我們可以非常容易地使用前一個例子中的一系列命令來分析日誌檔，這並非巧合：事實上，這實際上是Unix的關鍵設計思想之一，且它今天仍然令人訝異地關聯。讓我們更深入地研究一下，以便從Unix中借鑒一些想法【10】。

   Unix管道的發明者道格·麥克羅伊（Doug McIlroy）在1964年首先描述了這種情況【11】：“當我們需要將消息從一個程式傳遞另一個程式時，我們需要一種類似水管法蘭的拼接程式的方式【a】 ，I/O應該也按照這種方式進行“。水管的類比仍然在生效，通過管道連接程式的想法成為了現在被稱為**Unix哲學**的一部分 —— 這一組設計原則在Unix用戶與開發者之間流行起來，該哲學在1978年表述如下【12,13】：

1. 讓每個程式都做好一件事。要做一件新的工作，寫一個新程式，而不是通過添加“功能”讓老程式複雜化。
2. 期待每個程式的輸出成為另一個程式的輸入。不要將無關資訊混入輸出。避免使用嚴格的列資料或二進位輸入格式。不要堅持互動式輸入。
3. 設計和構建軟體，甚至是作業系統，要儘早嘗試，最好在幾周內完成。不要猶豫，扔掉笨拙的部分，重建它們。
4. 優先使用工具來減輕程式設計任務，即使必須曲線救國編寫工具，且在用完後很可能要扔掉大部分。

這種方法 —— 自動化，快速原型設計，增量式反覆運算，對實驗友好，將大型專案分解成可管理的塊 —— 聽起來非常像今天的敏捷開發和DevOps運動。奇怪的是，四十年來變化不大。

   `sort`工具是一個很好的例子。可以說它比大多數程式設計語言標準庫中的實現（它們不會利用磁片或使用多執行緒，即使這樣做有很大好處）要更好。然而，單獨使用`sort` 幾乎沒什麼用。它只能與其他Unix工具（如`uniq`）結合使用。

   像 `bash`這樣的Unix shell可以讓我們輕鬆地將這些小程式組合成令人訝異的強大資料處理任務。儘管這些程式中有很多是由不同人群編寫的，但它們可以靈活地結合在一起。 Unix如何實現這種可組合性？

#### 統一的介面

   如果你希望一個程式的輸出成為另一個程式的輸入，那意味著這些程式必須使用相同的資料格式 —— 換句話說，一個相容的介面。如果你希望能夠將任何程式的輸出連接到任何程式的輸入，那意味著所有程式必須使用相同的I/O介面。

   在Unix中，這種介面是一個**檔（file）**（更準確地說，是一個檔描述符）。一個檔只是一串有序的位元組序列。因為這是一個非常簡單的介面，所以可以使用相同的介面來表示許多不同的東西：檔案系統上的真實檔，到另一個進程（Unix通訊端，stdin，stdout）的通信通道，設備驅動程式（比如`/dev/audio`或`/dev/lp0`），表示TCP連接的通訊端等等。很容易將這些設計視為理所當然的，但實際上能讓這些差異巨大的東西共用一個統一的介面是非常厲害的，這使得它們可以很容易地連接在一起[^ii]。

[^ii]: 統一介面的另一個例子是URL和HTTP，這是Web的基石。 一個URL標識一個網站上的一個特定的東西（資源），你可以連結到任何其他網站的任何網址。 具有網路流覽器的使用者因此可以通過跟隨連結在網站之間無縫跳轉，即使伺服器可能由完全不相關的組織維護。 這個原則現在似乎非常明顯，但它卻是網路取能取得今天成就的關鍵。 之前的系統並不是那麼統一：例如，在公告板系統（BBS）時代，每個系統都有自己的電話號碼和串列傳輸速率配置。 從一個BBS到另一個BBS的引用必須以電話號碼和數據機設置的形式；用戶將不得不掛斷，撥打其他BBS，然後手動找到他們正在尋找的資訊。 這是不可能的直接連結到另一個BBS內的一些內容。

   按照慣例，許多（但不是全部）Unix程式將這個位元組序列視為ASCII文本。我們的日誌分析示例使用了這個事實：`awk`，`sort`，`uniq`和`head`都將它們的輸入檔視為由`\n`（分行符號，ASCII `0x0A`）字元分隔的記錄清單。 `\n`的選擇是任意的 —— 可以說，ASCII記錄分隔符號`0x1E`本來就是一個更好的選擇，因為它是為了這個目的而設計的【14】，但是無論如何，所有這些程式都使用相同的記錄分隔符號允許它們交互操作。

   每條記錄（即一行輸入）的解析則更加模糊。 Unix工具通常通過空白或定位字元將行分割成欄位，但也使用CSV（逗號分隔），管道分隔和其他編碼。即使像`xargs`這樣一個相當簡單的工具也有六個命令列選項，用於指定如何解析輸入。

   ASCII文本的統一介面大多數時候都能工作，但它不是很優雅：我們的日誌分析示例使用`{print $7}`來提取網址，這樣可讀性不是很好。在理想的世界中可能是`{print $request_url}`或類似的東西。我們稍後會回顧這個想法。

   儘管幾十年後還不夠完美，但統一的Unix介面仍然是非常出色的設計。沒有多少軟體能像Unix工具一樣交互組合的這麼好：你不能通過自訂分析工具輕鬆地將電子郵件帳戶的內容和線上購物歷史記錄以管道傳送至試算表中，並將結果發佈到社交網路或維琪。今天，像Unix工具一樣流暢地運行程式是一種例外，而不是規範。

   即使是具有**相同資料模型**的資料庫，將資料從一種匯出再導入另一種也並不容易。缺乏整合導致了資料的**巴爾幹化**[^譯注i]。

[^譯注i]: **巴爾幹化（Balkanization）**是一個常帶有貶義的地緣政治學術語，其定義為：一個國家或政區分裂成多個互相敵對的國家或政區的過程。


#### 邏輯與佈線相分離

   Unix工具的另一個特點是使用標準輸入（`stdin`）和標準輸出（`stdout`）。如果你運行一個程式，而不指定任何其他的東西，標準輸入來自鍵盤，標準輸出指向螢幕。但是，你也可以從檔輸入和/或將輸出重定向到檔。管道允許你將一個進程的標準輸出附加到另一個進程的標準輸入（有個小記憶體緩衝區，而不需要將整個中間資料流程寫入磁片）。

   程式仍然可以直接讀取和寫入檔，但如果程式不擔心特定的檔路徑，只使用標準輸入和標準輸出，則Unix方法效果最好。這允許shell使用者以任何他們想要的方式連接輸入和輸出；該程式不知道或不關心輸入來自哪裡以及輸出到哪裡。 （人們可以說這是一種**松耦合（loose coupling）**，**晚期繫結（late binding）**【15】或**控制反轉（inversion of control）**【16】）。將輸入/輸出佈線與程式邏輯分開，可以將小工具組合成更大的系統。

   你甚至可以編寫自己的程式，並將它們與作業系統提供的工具組合在一起。你的程式只需要從標準輸入讀取輸入，並將輸出寫入標準輸出，它就可以加入資料處理的管道中。在日誌分析示例中，你可以編寫一個將Usage-Agent字串轉換為更靈敏的流覽器識別字，或者將IP位址轉換為國家代碼的工具，並將其插入管道。`sort`程式並不關心它是否與作業系統的另一部分或者你寫的程式通信。

   但是，使用`stdin`和`stdout`能做的事情是有限的。需要多個輸入或輸出的程式是可能的，但非常棘手。你沒法將程式的輸出管道連接至網路連接中【17,18】[^iii] 。如果程式直接打開檔進行讀取和寫入，或者將另一個程式作為子進程啟動，或者打開網路連接，那麼I/O的佈線就取決於程式本身了。它仍然可以被配置（例如通過命令列選項），但在Shell中對輸入和輸出進行佈線的靈活性就少了。

[^iii]: 除了使用一個單獨的工具，如`netcat`或`curl`。 Unix開始試圖將所有東西都表示為檔，但是BSD通訊端API偏離了這個慣例【17】。研究用作業系統Plan 9和Inferno在使用檔方面更加一致：它們將TCP連接表示為`/net/tcp`中的檔【18】。


#### 透明度和實驗

使Unix工具如此成功的部分原因是，它們使查看正在發生的事情變得非常容易：

- Unix命令的輸入檔通常被視為不可變的。這意味著你可以隨意運行命令，嘗試各種命令列選項，而不會損壞輸入檔。

- 你可以在任何時候結束管道，將管道輸出到`less`，然後查看它是否具有預期的形式。這種檢查能力對調試非常有用。
- 你可以將一個流水線階段的輸出寫入檔，並將該檔用作下一階段的輸入。這使你可以重新啟動後面的階段，而無需重新運行整個管道。

因此，與關聯式資料庫的查詢最佳化工具相比，即使Unix工具非常簡單，但仍然非常有用，特別是對於實驗而言。

然而，Unix工具的最大局限在於它們只能在一台機器上運行 —— 而Hadoop這樣的工具即應運而生。


## MapReduce和分散式檔案系統

   MapReduce有點像Unix工具，但分佈在數千台機器上。像Unix工具一樣，它相當簡單粗暴，但令人驚異地管用。一個MapReduce作業可以和一個Unix進程相類比：它接受一個或多個輸入，並產生一個或多個輸出。

   和大多數Unix工具一樣，運行MapReduce作業通常不會修改輸入，除了生成輸出外沒有任何副作用。輸出檔以連續的方式一次性寫入（一旦寫入檔，不會修改任何現有的檔部分）。

   雖然Unix工具使用`stdin`和`stdout`作為輸入和輸出，但MapReduce作業在分散式檔案系統上讀寫文件。在Hadoop的Map-Reduce實現中，該檔案系統被稱為**HDFS（Hadoop分散式檔案系統）**，一個Google檔案系統（GFS）的開源實現【19】。

   除HDFS外，還有各種其他分散式檔案系統，如GlusterFS和Quantcast File System（QFS）【20】。諸如Amazon S3，Azure Blob存儲和OpenStack Swift 【21】等物件存儲服務在很多方面都是相似的[^iv]。在本章中，我們將主要使用HDFS作為示例，但是這些原則適用於任何分散式檔案系統。

[^iv]: 一個不同之處在於，對於HDFS，可以將計算任務安排在存儲特定檔副本的電腦上運行，而物件存儲通常將存儲和計算分開。如果網路頻寬是一個瓶頸，從本地磁片讀取有性能優勢。但是請注意，如果使用糾刪碼，則會丟失局部性，因為來自多台機器的資料必須進行合併以重建原始檔【20】。

   與網路連接存儲（NAS）和存儲區域網路（SAN）架構的共用磁片方法相比，HDFS基於**無共用**原則（參見[第二部分前言](part-ii.md)）。共用磁片存儲由集中式存放裝置實現，通常使用定制硬體和私人網路絡基礎設施（如光纖通道）。而另一方面，無共用方法不需要特殊的硬體，只需要通過傳統資料中心網路連接的電腦。

   HDFS包含在每台機器上運行的守護進程，對外暴露網路服務，允許其他節點訪問存儲在該機器上的檔（假設資料中心中的每台通用電腦都掛載著一些磁片）。名為**NameNode**的中央伺服器會跟蹤哪個檔塊存儲在哪台機器上。因此，HDFS在概念上創建了一個大型檔案系統，可以使用所有運行有守護進程的機器的磁片。

   為了容忍機器和磁片故障，檔塊被複製到多台機器上。複製可能意味著多個機器上的相同資料的多個副本，如[第5章](ch5.md)中所述，或者諸如Reed-Solomon碼這樣的糾刪碼方案，它允許以比完全複製更低的存儲開銷以恢復丟失的資料【20,22】。這些技術與RAID相似，可以在連接到同一台機器的多個磁片上提供冗餘；區別在於在分散式檔案系統中，檔訪問和複製是在傳統的資料中心網路上完成的，沒有特殊的硬體。

   HDFS已經擴展的很不錯了：在撰寫本書時，最大的HDFS部署運行在上萬台機器上，總存儲容量達數百PB【23】。如此大的規模已經變得可行，因為使用商品硬體和開源軟體的HDFS上的資料存儲和訪問成本遠低於專用存放裝置上的同等容量【24】。

### MapReduce作業執行

   MapReduce是一個程式設計框架，你可以使用它編寫代碼來處理HDFS等分散式檔案系統中的大型資料集。理解它的最簡單方法是參考“[簡單日誌分析](#簡單日誌分析)”中的Web伺服器日誌分析示例。MapReduce中的資料處理模式與此示例非常相似：

1. 讀取一組輸入檔，並將其分解成**記錄（records）**。在Web伺服器日誌示例中，每條記錄都是日誌中的一行（即`\n`是記錄分隔符號）。
2. 調用Mapper函數，從每條輸入記錄中提取一對鍵值。在前面的例子中，Mapper函數是`awk '{print $7}'`：它提取URL（`$7`）作為關鍵字，並將值留空。
3. 按鍵排序所有的鍵值對。在日誌的例子中，這由第一個`sort`命令完成。
4. 調用Reducer函數遍歷排序後的鍵值對。如果同一個鍵出現多次，排序使它們在列表中相鄰，所以很容易組合這些值而不必在記憶體中保留很多狀態。在前面的例子中，Reducer是由`uniq -c`命令實現的，該命令使用相同的鍵來統計相鄰記錄的數量。

這四個步驟可以作為一個MapReduce作業執行。步驟2（Map）和4（Reduce）是你編寫自訂資料處理代碼的地方。步驟1（將檔分解成記錄）由輸入格式解析器處理。步驟3中的排序步驟隱含在MapReduce中 —— 你不必編寫它，因為Mapper的輸出始終在送往Reducer之前進行排序。

要創建MapReduce作業，你需要實現兩個回呼函數，Mapper和Reducer，其行為如下（參閱“[MapReduce查詢](ch2.md#MapReduce查詢)”）：

***Mapper***

   Mapper會在每條輸入記錄上調用一次，其工作是從輸入記錄中提取鍵值。對於每個輸入，它可以生成任意數量的鍵值對（包括None）。它不會保留從一個輸入記錄到下一個記錄的任何狀態，因此每個記錄都是獨立處理的。

***Reducer***
    MapReduce框架拉取由Mapper生成的鍵值對，收集屬於同一個鍵的所有值，並使用在這組值列表上反覆運算調用Reducer。 Reducer可以產生輸出記錄（例如相同URL的出現次數）。

   在Web伺服器日誌的例子中，我們在第5步中有第二個`sort`命令，它按請求數對URL進行排序。在MapReduce中，如果你需要第二個排序階段，則可以通過編寫第二個MapReduce作業並將第一個作業的輸出用作第二個作業的輸入來實現它。這樣看來，Mapper的作用是將資料放入一個適合排序的表單中，並且Reducer的作用是處理已排序的資料。

#### 分散式執行MapReduce

   MapReduce與Unix命令管道的主要區別在於，MapReduce可以在多台機器上並存執行計算，而無需編寫代碼來顯式處理並行問題。Mapper和Reducer一次只能處理一條記錄；它們不需要知道它們的輸入來自哪裡，或者輸出去往什麼地方，所以框架可以處理在機器之間移動資料的複雜性。

   在分散式運算中可以使用標準的Unix工具作為Mapper和Reducer【25】，但更常見的是，它們被實現為傳統程式設計語言的函數。在Hadoop MapReduce中，Mapper和Reducer都是實現特定介面的Java類。在MongoDB和CouchDB中，Mapper和Reducer都是JavaScript函數（參閱“[MapReduce查詢](ch2.md#MapReduce查詢)”）。

   [圖10-1]()顯示了Hadoop MapReduce作業中的資料流程。其並行化基於分區（參見[第6章](ch6.md)）：作業的輸入通常是HDFS中的一個目錄，輸入目錄中的每個檔或檔塊都被認為是一個單獨的分區，可以單獨處理map任務（[圖10-1](img/fig10-1.png)中的m1，m2和m3標記）。

   每個輸入檔的大小通常是數百百萬位元組。 MapReduce調度器（圖中未顯示）試圖在其中一台存儲輸入檔副本的機器上運行每個Mapper，只要該機器有足夠的備用RAM和CPU資源來運行Mapper任務【26】。這個原則被稱為**將計算放在資料附近**【27】：它節省了通過網路複製輸入檔的開銷，減少網路負載並增加局部性。

![](img/fig10-1.png)

**圖10-1 具有三個Mapper和三個Reducer的MapReduce任務**

   在大多數情況下，應該在Mapper任務中運行的應用代碼在將要運行它的機器上還不存在，所以MapReduce框架首先將代碼（例如Java程式中的JAR檔）複製到適當的機器。然後啟動Map任務並開始讀取輸入檔，一次將一條記錄傳入Mapper回呼函數。Mapper的輸出由鍵值對組成。

   計算的Reduce端也被分區。雖然Map任務的數量由輸入檔塊的數量決定，但Reducer的任務的數量是由作業作者配置的（它可以不同於Map任務的數量）。為了確保具有相同鍵的所有鍵值對最終落在相同的Reducer處，框架使用鍵的散列值來確定哪個Reduce任務應該接收到特定的鍵值對（參見“[按鍵散列分區](ch6.md#按鍵散列分區)”））。

   鍵值對必須進行排序，但資料集可能太大，無法在單台機器上使用常規排序演算法進行排序。相反，分類是分階段進行的。首先每個Map任務都按照Reducer對輸出進行分區。每個分區都被寫入Mapper程式的本地磁片，使用的技術與我們在“[SSTables與LSM樹](ch3.md#SSTables與LSM樹)”中討論的類似。

   只要當Mapper讀取完輸入檔，並寫完排序後的輸出檔，MapReduce調度器就會通知Reducer可以從該Mapper開始獲取輸出檔。Reducer連接到每個Mapper，並下載自己相應分區的有序鍵值對文件。按Reducer分區，排序，從Mapper向Reducer複製分區資料，這一整個過程被稱為**混洗（shuffle）**【26】（一個容易混淆的術語  —— 不像洗牌，在MapReduce中的混洗沒有隨機性）。

   Reduce任務從Mapper獲取檔，並將它們合併在一起，並保留有序特性。因此，如果不同的Mapper生成了鍵相同的記錄，則在Reducer的輸入中，這些記錄將會相鄰。

   Reducer調用時會收到一個鍵，和一個反覆運算器作為參數，反覆運算器會順序地掃過所有具有該鍵的記錄（因為在某些情況可能無法完全放入記憶體中）。Reducer可以使用任意邏輯來處理這些記錄，並且可以生成任意數量的輸出記錄。這些輸出記錄會寫入分散式檔案系統上的檔中（通常是在跑Reducer的機器本地磁片上留一份，並在其他機器上留幾份副本）。

#### MapReduce工作流

   單個MapReduce作業可以解決的問題範圍很有限。以日誌分析為例，單個MapReduce作業可以確定每個URL的頁面流覽次數，但無法確定最常見的URL，因為這需要第二輪排序。

   因此將MapReduce作業連結成為**工作流（workflow）**中是極為常見的，例如，一個作業的輸出成為下一個作業的輸入。 Hadoop Map-Reduce框架對工作流沒有特殊支持，所以這個鏈是通過目錄名隱式實現的：第一個作業必須將其輸出配置為HDFS中的指定目錄，第二個作業必須將其輸入配置為從同一個目錄。從MapReduce框架的角度來看，這是是兩個獨立的作業。

   因此，被連結的MapReduce作業並沒有那麼像Unix命令管道（它直接將一個進程的輸出作為另一個進程的輸入，僅用一個很小的記憶體緩衝區）。它更像是一系列命令，其中每個命令的輸出寫入暫存檔案，下一個命令從暫存檔案中讀取。這種設計有利也有弊，我們將在“[物化中間狀態](#物化中間狀態)”中討論。

   只有當作業成功完成後，批次處理作業的輸出才會被視為有效的（MapReduce會丟棄失敗作業的部分輸出）。因此，工作流中的一項作業只有在先前的作業 —— 即生產其輸入的作業 —— 成功完成後才能開始。為了處理這些作業之間的依賴，有很多針對Hadoop的工作流調度器被開發出來，包括Oozie，Azkaban，Luigi，Airflow和Pinball 【28】。

   這些調度程式還具有管理功能，在維護大量批次處理作業時非常有用。在構建推薦系統時，由50到100個MapReduce作業組成的工作流是常見的【29】。而在大型組織中，許多不同的團隊可能運行不同的作業來讀取彼此的輸出。工具支持對於管理這樣複雜的資料流程而言非常重要。

   Hadoop的各種高級工具（如Pig 【30】，Hive 【31】，Cascading 【32】，Crunch 【33】和FlumeJava 【34】）也能自動佈線組裝多個MapReduce階段，生成合適的工作流。

### Reduce端連接與分組

   我們在[第2章](ch2.md)中討論了資料模型和查詢語言的聯接，但是我們還沒有深入探討連接是如何實現的。現在是我們再次撿起這條線索的時候了。

   在許多資料集中，一條記錄與另一條記錄存在關聯是很常見的：關係模型中的**外鍵**，文檔模型中的**文檔引用**或圖模型中的**邊**。當你需要同時訪問這一關聯的兩側（持有引用的記錄與被引用的記錄）時，連接就是必須的。（包含引用的記錄和被引用的記錄），連接就是必需的。正如[第2章](ch2.md)所討論的，非規範化可以減少對連接的需求，但通常無法將其完全移除[^v]。

[^v]: 我們在本書中討論的連接通常是等值連接，即最常見的連接類型，其中記錄與其他記錄在特定欄位（例如ID）中具有**相同值**相關聯。有些資料庫支援更通用的連接類型，例如使用小於運算子而不是等號運算子，但是我們沒有地方來講這些東西。

   在資料庫中，如果執行只涉及少量記錄的查詢，資料庫通常會使用**索引**來快速定位感興趣的記錄（參閱[第3章](ch3.md)）。如果查詢涉及到連接，則可能涉及到查找多個索引。然而MapReduce沒有索引的概念 —— 至少在通常意義上沒有。

   當MapReduce作業被賦予一組檔作為輸入時，它讀取所有這些檔的全部內容；資料庫會將這種操作稱為**全資料表掃描**。如果你只想讀取少量的記錄，則全資料表掃描與索引查詢相比，代價非常高昂。但是在分析查詢中（參閱“[交易處理或分析？](ch3.md#交易處理還是分析？)”），通常需要計算大量記錄的聚合。在這種情況下，特別是如果能在多台機器上並行處理時，掃描整個輸入可能是相當合理的事情。

   當我們在批次處理的語境中討論連接時，我們指的是在資料集中解析某種關聯的全量存在。 例如我們假設一個作業是同時處理所有使用者的資料，而非僅僅是為某個特定使用者查找資料（而這能通過索引更高效地完成）。

#### 示例：分析用戶活動事件

   [圖10-2](img/fig10-2.png)給出了一個批次處理作業中連接的典型例子。左側是事件日誌，描述登錄使用者在網站上做的事情（稱為**活動事件（activity events）**或**點選流向資料（clickstream data）**），右側是使用者資料庫。 你可以將此示例看作是星型模式的一部分（參閱“[星型和雪花型：分析的模式](ch3.md#星型和雪花型：分析的模式)”）：事件日誌是事實表，使用者資料庫是其中的一個維度。

![](img/fig10-2.png)

**圖10-2 使用者行為日誌與使用者檔案的連接**

   分析任務可能需要將使用者活動與使用者簡檔相關聯：例如，如果檔案包含使用者的年齡或出生日期，系統就可以確定哪些頁面更受哪些年齡段的用戶歡迎。然而活動事件僅包含用戶ID，而沒有包含完整的使用者檔案資訊。在每個活動事件中嵌入這些檔案資訊很可能會非常浪費。因此，活動事件需要與使用者檔案資料庫相連接。

   實現這一連接的最簡單方法是，逐個遍歷活動事件，並為每個遇到的使用者ID查詢使用者資料庫（在遠端伺服器上）。這是可能的，但是它的性能可能會非常差：處理輸送量將受限於受資料庫伺服器的往返時間，本地緩存的有效性很大程度上取決於資料的分佈，並行運行大量查詢可能會輕易壓垮資料庫【35】。

   為了在批次處理過程中實現良好的輸送量，計算必須（盡可能）限於單台機器上進行。為待處理的每條記錄發起隨機訪問的網路請求實在是太慢了。而且，查詢遠端資料庫意味著批次處理作業變為**非確定的（nondeterministic）**，因為遠端資料庫中的資料可能會改變。

   因此，更好的方法是獲取使用者資料庫的副本（例如，使用ETL進程從資料庫備份中提取資料，參閱“[資料倉庫](ch3.md#資料倉庫)”），並將它和使用者行為日誌放入同一個分散式檔案系統中。然後你可以將使用者資料庫存儲在HDFS中的一組檔中，而使用者活動記錄存儲在另一組檔中，並能用MapReduce將所有相關記錄集中到同一個地方進行高效處理。

#### 排序合併連接

   回想一下，Mapper的目的是從每個輸入記錄中提取一對鍵值。在[圖10-2](img/fig10-2.png)的情況下，這個鍵就是用戶ID：一組Mapper會掃過活動事件（提取用戶ID作為鍵，活動事件作為值），而另一組Mapper將會掃過使用者資料庫（提取使用者ID作為鍵，用戶的出生日期作為值）。這個過程如[圖10-3](img/fig10-3.png)所示。

![](img/fig10-3.png)

**圖10-3 在用戶ID上進行的Reduce端連接。如果輸入資料集分區為多個檔，則每個分區都會被多個Mapper並行處理**

   當MapReduce框架通過鍵對Mapper輸出進行分區，然後對鍵值對進行排序時，效果是具有相同ID的所有活動事件和使用者記錄在Reducer輸入中彼此相鄰。 Map-Reduce作業甚至可以也讓這些記錄排序，使Reducer總能先看到來自使用者資料庫的記錄，緊接著是按時間戳記順序排序的活動事件 ——  這種技術被稱為**二次排序（secondary sort）**【26】。

   然後Reducer可以容易地執行實際的連接邏輯：每個使用者ID都會被調用一次Reducer函數，且因為二次排序，第一個值應該是來自使用者資料庫的出生日期記錄。 Reducer將出生日期存儲在區域變數中，然後使用相同的用戶ID遍歷活動事件，輸出**已觀看網址**和**觀看者年齡**的結果對。隨後的Map-Reduce作業可以計算每個URL的查看者年齡分佈，並按年齡段進行聚集。

   由於Reducer一次處理一個特定使用者ID的所有記錄，因此一次只需要將一條使用者記錄保存在記憶體中，而不需要通過網路發出任何請求。這個演算法被稱為**排序合併連接（sort-merge join）**，因為Mapper的輸出是按鍵排序的，然後Reducer將來自連接兩側的有序記錄清單合併在一起。

#### 把相關資料放在一起

   在排序合併連接中，Mapper和排序過程確保了所有對特定用戶ID執行連接操作的必須資料都被放在同一個地方：單次調用Reducer的地方。預先排好了所有需要的資料，Reducer可以是相當簡單的單執行緒代碼，能夠以高輸送量和與低記憶體開銷掃過這些記錄。

   這種架構可以看做，Mapper將“消息”發送給Reducer。當一個Mapper發出一個鍵值對時，這個鍵的作用就像值應該傳遞到的目標位址。即使鍵只是一個任意的字串（不是像IP位址和埠號那樣的實際的網路位址），它表現的就像一個位址：所有具有相同鍵的鍵值對將被傳遞到相同的目標（一次Reduce的調用）。

   使用MapReduce程式設計模型，能將計算的物理網路通信層面（從正確的機器獲取資料）從應用邏輯中剝離出來（獲取資料後執行處理）。這種分離與資料庫的典型用法形成了鮮明對比，從資料庫中獲取資料的請求經常出現在應用代碼內部【36】。由於MapReduce能夠處理所有的網路通信，因此它也避免了應用代碼去擔心部分故障，例如另一個節點的崩潰：MapReduce在不影響應用邏輯的情況下能透明地重試失敗的任務。

### GROUP BY

   除了連接之外，“把相關資料放在一起”的另一種常見模式是，按某個鍵對記錄分組（如SQL中的GROUP BY子句）。所有帶有相同鍵的記錄構成一個組，而下一步往往是在每個組內進行某種聚合操作，例如：

- 統計每個組中記錄的數量（例如在統計PV的例子中，在SQL中表示為`COUNT(*)`聚合）
- 對某個特定欄位求和（SQL中的`SUM(fieldname)`）
- 按某種分級函數取出排名前k條記錄。

使用MapReduce實現這種分組操作的最簡單方法是設置Mapper，以便它們生成的鍵值對使用所需的分組鍵。然後分區和排序過程將所有具有相同分區鍵的記錄導向同一個Reducer。因此在MapReduce之上實現分組和連接看上去非常相似。

   分組的另一個常見用途是整理特定用戶會話的所有活動事件，以找出用戶進行的一系列操作（稱為**會話化（sessionization）**【37】）。例如，可以使用這種分析來確定顯示新版網站的使用者是否比那些顯示舊版本（A/B測試）的用戶更有購買欲，或者計算某個行銷活動是否值得。

   如果你有多個Web伺服器處理用戶請求，則特定用戶的活動事件很可能分散在各個不同的伺服器的日誌檔中。你可以通過使用會話cookie，用戶ID或類似的識別字作為分組鍵，以將特定用戶的所有活動事件放在一起來實現會話化，與此同時，不同用戶的事件仍然散步在不同的分區中。

#### 處理傾斜

   如果存在與單個鍵關聯的大量資料，則“將具有相同鍵的所有記錄放到相同的位置”這種模式就被破壞了。例如在社交網路中，大多數使用者可能會與幾百人有連接，但少數名人可能有數百萬的追隨者。這種不成比例的活動資料庫記錄被稱為**關鍵物件（linchpin object）**【38】或**熱鍵（hot key）**。

   在單個Reducer中收集與某個名流相關的所有活動（例如他們發佈內容的回復）可能導致嚴重的傾斜（也稱為**熱點（hot spot）**）—— 也就是說，一個Reducer必須比其他Reducer處理更多的記錄（參見“[負載傾斜與消除熱點](ch6.md#負載傾斜與消除熱點)“）。由於MapReduce作業只有在所有Mapper和Reducer都完成時才完成，所有後續作業必須等待最慢的Reducer才能啟動。

   如果連接的輸入存在熱點鍵，可以使用一些演算法進行補償。例如，Pig中的**傾斜連接（skewed join）**方法首先運行一個抽樣作業來確定哪些鍵是熱鍵【39】。連接實際執行時，Mapper會將熱鍵的關聯記錄**隨機**（相對于傳統MapReduce基於鍵散列的確定性方法）發送到幾個Reducer之一。對於另外一側的連接輸入，與熱鍵相關的記錄需要被複製到所有處理該鍵的Reducer上【40】。

   這種技術將處理熱鍵的工作分散到多個Reducer上，這樣可以使其更好地並行化，代價是需要將連接另一側的輸入記錄複製到多個Reducer上。 Crunch中的**分片連接（sharded join）**方法與之類似，但需要顯式指定熱鍵而不是使用採樣作業。這種技術也非常類似於我們在“[負載傾斜與消除熱點](ch6.md#負載傾斜與消除熱點)”中討論的技術，使用隨機化來緩解分區資料庫中的熱點。

   Hive的偏斜連接優化採取了另一種方法。它需要在表格中繼資料中顯式指定熱鍵，並將與這些鍵相關的記錄單獨存放，與其它檔分開。當在該表上執行連接時，對於熱鍵，它會使用Map端連接（參閱[下一節](#Map端連接)）。

   當按照熱鍵進行分組並聚合時，可以將分組分兩個階段進行。第一個MapReduce階段將記錄發送到隨機Reducer，以便每個Reducer只對熱鍵的子集執行分組，為每個鍵輸出一個更緊湊的中間聚合結果。然後第二個MapReduce作業將所有來自第一階段Reducer的中間聚合結果合併為每個鍵一個值。


### Map端連接

   上一節描述的連接演算法在Reducer中執行實際的連接邏輯，因此被稱為Reduce端連接。Mapper扮演著預處理輸入資料的角色：從每個輸入記錄中提取鍵值，將鍵值對分配給Reducer分區，並按鍵排序。

   Reduce端方法的優點是不需要對輸入資料做任何假設：無論其屬性和結構如何，Mapper都可以對其預處理以備連接。然而不利的一面是，排序，複製至Reducer，以及合併Reducer輸入，所有這些操作可能開銷巨大。當資料通過MapReduce 階段時，資料可能需要落盤好幾次，取決於可用的記憶體緩衝區【37】。

   另一方面，如果你**能**對輸入資料作出某些假設，則通過使用所謂的Map端連接來加快連線速度是可行的。這種方法使用了一個閹掉Reduce與排序的MapReduce作業，每個Mapper只是簡單地從分散式檔案系統中讀取一個輸入檔塊，然後將輸出檔寫入檔案系統，僅此而已。

#### 廣播散列連接

   適用於執行Map端連接的最簡單場景是大資料集與小資料集連接的情況。要點在於小資料集需要足夠小，以便可以將其全部載入到每個Mapper的記憶體中。

   例如，假設在[圖10-2](img/fig10-2.png)的情況下，使用者資料庫小到足以放進記憶體中。在這種情況下，當Mapper啟動時，它可以首先將使用者資料庫從分散式檔案系統讀取到記憶體中的散列中。完成此操作後，Map程式可以掃描使用者活動事件，並簡單地在散列表中查找每個事件的用戶ID[^vi]。

[^vi]: 這個例子假定散列表中的每個鍵只有一個條目，這對使用者資料庫（使用者ID唯一標識一個使用者）可能是正確的。通常，雜湊表可能需要包含具有相同鍵的多個條目，而連接運算子將對每個鍵輸出所有的匹配。

   參與連接的較大輸入的每個檔塊各有一個Mapper（在[圖10-2](img/fig10-2.png)的例子中活動事件是較大的輸入）。每個Mapper都會將較小輸入整個載入到記憶體中。

   這種簡單有效的演算法被稱為**廣播散列連接（broadcast hash join）**：**廣播**一詞反映了這樣一個事實，每個連接較大輸入端分區的Mapper都會將較小輸入端資料集整個讀入記憶體中（所以較小輸入實際上“廣播”到較大資料的所有分區上），**散列**一詞反映了它使用一個散列表。 Pig（名為“**複製連結（replicated join）**”），Hive（“**MapJoin**”），Cascading和Crunch支持這種連接。它也被諸如Impala的資料倉庫查詢引擎使用【41】。

   除了將連接較小輸入載入到記憶體散清單中，另一種方法是將較小輸入存儲在本地磁片上的唯讀索引中【42】。索引中經常使用的部分將保留在作業系統的頁面緩存中，因而這種方法可以提供與記憶體散清單幾乎一樣快的隨機查找性能，但實際上並不需要資料集能放入記憶體中。

#### 分區散列連接

   如果Map端連接的輸入以相同的方式進行分區，則散列連接方法可以獨立應用於每個分區。在[圖10-2](img/fig10-2.png)的情況中，你可以根據使用者ID的最後一位十進位數字字來對活動事件和使用者資料庫進行分區（因此連接兩側各有10個分區）。例如，Mapper3首先將所有具有以3結尾的ID的用戶載入到散清單中，然後掃描ID為3的每個用戶的所有活動事件。

   如果分區正確無誤，可以確定的是，所有你可能需要連接的記錄都落在同一個編號的分區中。因此每個Mapper只需要從輸入兩端各讀取一個分區就足夠了。好處是每個Mapper都可以在記憶體散清單中少放點數據。

   這種方法只有當連接兩端輸入有相同的分區數，且兩側的記錄都是使用相同的鍵與相同的雜湊函數做分區時才適用。如果輸入是由之前執行過這種分組的MapReduce作業生成的，那麼這可能是一個合理的假設。

   分區散列連接在Hive中稱為**Map端桶連接（bucketed map joins）【37】**。

#### Map端合併連接

   如果輸入資料集不僅以相同的方式進行分區，而且還基於相同的鍵進行**排序**，則可適用另一種Map端聯接的變體。在這種情況下，輸入是否小到能放入記憶體並不重要，因為這時候Mapper同樣可以執行歸併操作（通常由Reducer執行）的歸併操作：按鍵遞增的順序依次讀取兩個輸入檔，將具有相同鍵的記錄配對。

   如果能進行Map端合併連接，這通常意味著前一個MapReduce作業可能一開始就已經把輸入資料做了分區並進行了排序。原則上這個連接就可以在前一個作業的Reduce階段進行。但使用獨立的僅Map作業有時也是合適的，例如，分好區且排好序的中間資料集可能還會用於其他目的。

#### MapReduce工作流與Map端連接

   當下游作業使用MapReduce連接的輸出時，選擇Map端連接或Reduce端連接會影響輸出的結構。Reduce端連接的輸出是按照**連接鍵**進行分區和排序的，而Map端連接的輸出則按照與較大輸入相同的方式進行分區和排序（因為無論是使用分區連接還是廣播連接，連接較大輸入端的每個檔塊都會啟動一個Map任務）。

   如前所述，Map端連接也對輸入資料集的大小，有序性和分區方式做出了更多假設。在優化連接策略時，瞭解分散式檔案系統中資料集的物理佈局變得非常重要：僅僅知道編碼格式和資料存儲目錄的名稱是不夠的；你還必須知道資料是按哪些鍵做的分區和排序，以及分區的數量。

   在Hadoop生態系統中，這種關於資料集分區的中繼資料通常在HCatalog和Hive Metastore中維護【37】。


### 批次處理工作流的輸出

   我們已經說了很多用於實現MapReduce工作流的演算法，但卻忽略了一個重要的問題：這些處理完成之後的最終結果是什麼？我們最開始為什麼要跑這些作業？

   在資料庫查詢的場景中，我們將交易處理（OLTP）與分析兩種目的區分開來（參閱“[交易處理或分析？](ch3.md#交易處理或分析？)”）。我們看到，OLTP查詢通常根據鍵查找少量記錄，使用索引，並將其呈現給用戶（比如在網頁上）。另一方面，分析查詢通常會掃描大量記錄，執行分組與聚合，輸出通常有著報告的形式：顯示某個指標隨時間變化的圖表，或按照某種排位取前10項，或一些數字細化為子類。這種報告的消費者通常是需要做出商業決策的分析師或經理。

   批次處理放哪裡合適？它不屬於交易處理，也不是分析。它和分析比較接近，因為批次處理通常會掃過輸入資料集的絕大部分。然而MapReduce作業工作流與用於分析目的的SQL查詢是不同的（參閱“[Hadoop與分散式資料庫的對比](#Hadoop與分散式資料庫的對比)”）。批次處理過程的輸出通常不是報表，而是一些其他類型的結構。

#### 建立搜索索引

   Google最初使用MapReduce是為其搜尋引擎建立索引，用了由5到10個MapReduce作業組成的工作流實現【1】。雖然Google後來也不僅僅是為這個目的而使用MapReduce 【43】，但如果從構建搜索索引的角度來看，更能幫助理解MapReduce。 （直至今日，Hadoop MapReduce仍然是為Lucene/Solr構建索引的好方法【44】）

   我們在“[全文檢索搜尋和模糊索引](ch3.md#全文檢索搜尋和模糊索引)”中簡要地瞭解了Lucene這樣的全文檢索搜尋索引是如何工作的：它是一個檔（關鍵字字典），你可以在其中高效地查找特定關鍵字，並找到包含該關鍵字的所有文檔ID列表（文章列表）。這是一種非常簡化的看法 —— 實際上，搜索索引需要各種額外資料，以便根據相關性對搜索結果進行排名，糾正拼寫錯誤，解析同義詞等等 —— 但這個原則是成立的。

   如果需要對一組固定文檔執行全文檢索搜尋，則批次處理是一種構建索引的高效方法：Mapper根據需要對文檔集合進行分區，每個Reducer構建該分區的索引，並將索引檔寫入分散式檔案系統。構建這樣的文檔分區索引（參閱“[分區和二級索引](ch6.md#分區和二級索引)”）並行處理效果拔群。

   由於按關鍵字查詢搜索索引是唯讀操作，因而這些索引檔一旦創建就是不可變的。

   如果索引的文檔集合發生更改，一種選擇是定期重跑整個索引工作流，並在完成後用新的索引檔批量替換以前的索引檔。如果只有少量的文檔發生了變化，這種方法的計算成本可能會很高。但它的優點是索引過程很容易理解：文檔進，索引出。

   另一個選擇是，可以增量建立索引。如[第3章](ch3.md)中討論的，如果要在索引中添加，刪除或更新文檔，Lucene會寫新的段檔，並在後臺非同步合併壓縮段檔。我們將在[第11章](ch11.md)中看到更多這種增量處理。

#### 鍵值存儲作為批次處理輸出

   搜索索引只是批次處理工作流可能輸出的一個例子。批次處理的另一個常見用途是構建機器學習系統，例如分類器（比如垃圾郵件篩檢程式，異常檢測，圖像識別）與推薦系統（例如，你可能認識的人，你可能感興趣的產品或相關的搜索【29】）。

   這些批次處理作業的輸出通常是某種資料庫：例如，可以通過給定用戶ID查詢該使用者推薦好友的資料庫，或者可以通過產品ID查詢相關產品的資料庫【45】。

   這些資料庫需要被處理使用者請求的Web應用所查詢，而它們通常是獨立於Hadoop基礎設施的。那麼批次處理過程的輸出如何回到Web應用可以查詢的資料庫中呢？

   最直接的選擇可能是，直接在Mapper或Reducer中使用你最愛資料庫的用戶端庫，並從批次處理作業直接寫入資料庫伺服器，一次寫入一條記錄。它能工作（假設你的防火牆規則允許從你的Hadoop環境直接訪問你的生產資料庫），但這並不是一個好主意，出於以下幾個原因：

- 正如前面在連接的上下文中討論的那樣，為每條記錄發起一個網路請求，要比批次處理任務的正常輸送量慢幾個數量級。即使用戶端庫支援批次處理，性能也可能很差。
- MapReduce作業經常並行運行許多工。如果所有Mapper或Reducer都同時寫入相同的輸出資料庫，並以批次處理的預期速率工作，那麼該資料庫很可能被輕易壓垮，其查詢性能可能變差。這可能會導致系統其他部分的運行問題【35】。
- 通常情況下，MapReduce為作業輸出提供了一個乾淨俐落的“全有或全無”保證：如果作業成功，則結果就是每個任務恰好執行一次所產生的輸出，即使某些任務失敗且必須一路重試。如果整個作業失敗，則不會生成輸出。然而從作業內部寫入外部系統，會產生外部可見的副作用，這種副作用是不能以這種方式被隱藏的。因此，你不得不去操心部分完成的作業對其他系統可見的結果，並需要理解Hadoop任務嘗試與預測執行的複雜性。

更好的解決方案是在批次處理作業**內**創建一個全新的資料庫，並將其作為檔寫入分散式檔案系統中作業的輸出目錄，就像上節中的搜索索引一樣。這些資料檔案一旦寫入就是不可變的，可以批量載入到處理唯讀查詢的伺服器中。不少鍵值存儲都支援在MapReduce作業中構建資料庫檔，包括Voldemort 【46】，Terrapin 【47】，ElephantDB 【48】和HBase批量載入【49】。

   構建這些資料庫檔是MapReduce的一種很好用法的使用方法：使用Mapper提取出鍵並按該鍵排序，現在已經是構建索引所必需的大量工作。由於這些鍵值存儲大多都是唯讀的（檔只能由批次處理作業一次性寫入，然後就不可變），所以資料結構非常簡單。比如它們就不需要WAL（參閱“[使B樹可靠](ch3.md#使B樹可靠)”）。

   將資料載入到Voldemort時，伺服器將繼續用舊資料檔案服務請求，同時將新資料檔案從分散式檔案系統複製到伺服器的本地磁片。一旦複製完成，伺服器會自動將查詢切換到新檔。如果在這個過程中出現任何問題，它可以輕易回滾至舊檔，因為它們仍然存在而且不可變【46】。

#### 批次處理輸出的哲學

   本章前面討論過的Unix哲學（“[Unix哲學](#Unix哲學)”）鼓勵以顯式指明資料流程的方式進行實驗：程式讀取輸入並寫入輸出。在這一過程中，輸入保持不變，任何先前的輸出都被新輸出完全替換，且沒有其他副作用。這意味著你可以隨心所欲地重新運行一個命令，略做改動或進行調試，而不會攪亂系統的狀態。

   MapReduce作業的輸出處理遵循同樣的原理。通過將輸入視為不可變且避免副作用（如寫入外部資料庫），批次處理作業不僅實現了良好的性能，而且更容易維護：

- 如果在代碼中引入了一個錯誤，而輸出錯誤或損壞了，則可以簡單地回滾到代碼的先前版本，然後重新運行該作業，輸出將重新被糾正。或者，甚至更簡單，你可以將舊的輸出保存在不同的目錄中，然後切換回原來的目錄。具有讀寫事務的資料庫沒有這個屬性：如果你部署了錯誤的代碼，將錯誤的資料寫入資料庫，那麼回滾代碼將無法修復資料庫中的資料。 （能夠從錯誤代碼中恢復的概念被稱為**人類容錯（human fault tolerance）**【50】）
- 由於回滾很容易，比起在錯誤意味著不可挽回的傷害的環境，功能開發進展能快很多。這種**最小化不可逆性（minimizing irreversibility）**的原則有利於敏捷軟體發展【51】。
- 如果Map或Reduce任務失敗，MapReduce框架將自動重新調度，並在同樣的輸入上再次運行它。如果失敗是由代碼中的錯誤造成的，那麼它會不斷崩潰，並最終導致作業在幾次嘗試之後失敗。但是如果故障是由於臨時問題導致的，那麼故障就會被容忍。因為輸入不可變，這種自動重試是安全的，而失敗任務的輸出會被MapReduce框架丟棄。
- 同一組檔可用作各種不同作業的輸入，包括計算指標的監控作業可以評估作業的輸出是否具有預期的性質（例如，將其與前一次運行的輸出進行比較並測量差異） 。
- 與Unix工具類似，MapReduce作業將邏輯與佈線（配置輸入和輸出目錄）分離，這使得關注點分離，可以重用代碼：一個團隊可以實現一個專注做好一件事的作業；而其他團隊可以決定何時何地運行這項作業。

在這些領域，在Unix上表現良好的設計原則似乎也適用於Hadoop，但Unix和Hadoop在某些方面也有所不同。例如，因為大多數Unix工具都假設輸入輸出是無類型文字檔，所以它們必須做大量的輸入解析工作（本章開頭的日誌分析示例使用`{print $7}`來提取URL）。在Hadoop上可以通過使用更結構化的檔案格式消除一些低價值的語法轉換：比如Avro（參閱“[Avro](ch4.md#Avro)”）和Parquet（參閱“[列存儲](ch3.md#列存儲)”）經常使用，因為它們提供了基於模式的高效編碼，並允許模式隨時間推移而演進（見第4章）。

### Hadoop與分散式資料庫的對比

   正如我們所看到的，Hadoop有點像Unix的分散式版本，其中HDFS是檔案系統，而MapReduce是Unix進程的怪異實現（總是在Map階段和Reduce階段運行`sort`工具）。我們瞭解了如何在這些原語的基礎上實現各種連接和分組操作。

   當MapReduce論文發表時【1】，它從某種意義上來說 —— 並不新鮮。我們在前幾節中討論的所有處理和並行連接演算法已經在十多年前所謂的**大規模並行處理（MPP， massively parallel processing）**資料庫中實現了【3,40】。比如Gamma database machine，Teradata和Tandem NonStop SQL就是這方面的先驅【52】。

   最大的區別是，MPP資料庫專注於在一組機器上並存執行分析SQL查詢，而MapReduce和分散式檔案系統【19】的組合則更像是一個可以運行任意程式的通用作業系統。

#### 存儲多樣性

   資料庫要求你根據特定的模型（例如關係或文檔）來構造資料，而分散式檔案系統中的檔只是位元組序列，可以使用任何資料模型和編碼來編寫。它們可能是資料庫記錄的集合，但同樣可以是文本，圖像，視頻，感測器讀數，疏鬆陣列，特徵向量，基因組序列或任何其他類型的資料。

   說白了，Hadoop開放了將資料不加區分地轉儲到HDFS的可能性，允許後續再研究如何進一步處理【53】。相比之下，在將資料導入資料庫專有存儲格式之前，MPP資料庫通常需要對資料和查詢模式進行仔細的前期建模。

   在純粹主義者看來，這種仔細的建模和導入似乎是可取的，因為這意味著資料庫的使用者有更高品質的資料來處理。然而實踐經驗表明，簡單地使資料快速可用 —— 即使它很古怪，難以使用，使用原始格式 —— 也通常要比事先決定理想資料模型要更有價值【54】。

   這個想法與資料倉庫類似（參閱“[資料倉庫](ch3.md#資料倉庫)”）：將大型組織的各個部分的資料集中在一起是很有價值的，因為它可以跨越以前相分離的資料集進行連接。 MPP資料庫所要求的謹慎模式設計拖慢了集中式資料收集速度；以原始形式收集資料，稍後再操心模式的設計，能使資料收集速度加快（有時被稱為“**資料湖（data lake）**”或“**企業資料中心（enterprise data hub）**”【55】）。

   不加區分的資料轉儲轉移了解釋資料的負擔：資料集的生產者不再需要強制將其轉化為標準格式，資料的解釋成為消費者的問題（**讀時模式**方法【56】；參閱“[文檔模型中的架構靈活性](ch2.md#文檔模型中的架構靈活性)”）。如果生產者和消費者是不同優先順序的不同團隊，這可能是一種優勢。甚至可能不存在一個理想的資料模型，對於不同目的有不同的合適視角。以原始形式簡單地轉儲資料，可以允許多種這樣的轉換。這種方法被稱為**壽司原則（sushi principle）**：“原始資料更好”【57】。

   因此，Hadoop經常被用於實現ETL過程（參閱“[資料倉庫](ch3.md#資料倉庫)”）：交易處理系統中的資料以某種原始形式轉儲到分散式檔案系統中，然後編寫MapReduce作業來清理資料，將其轉換為關係形式，並將其導入MPP資料倉庫以進行分析。資料建模仍然在進行，但它在一個單獨的步驟中進行，與資料收集相解耦。這種解耦是可行的，因為分散式檔案系統支援以任何格式編碼的資料。

#### 處理模型多樣性

   MPP資料庫是單體的，緊密集成的軟體，負責磁片上的存儲佈局，查詢計畫，調度和執行。由於這些元件都可以針對資料庫的特定需求進行調整和優化，因此整個系統可以在其設計針對的查詢類型上取得非常好的性能。而且，SQL查詢語言允許以優雅的語法表達查詢，而無需編寫代碼，使業務分析師用來做商業分析的視覺化工具（例如Tableau）能夠訪問。

   另一方面，並非所有類型的處理都可以合理地表達為SQL查詢。例如，如果要構建機器學習和推薦系統，或者使用相關性排名模型的全文檢索搜尋索引，或者執行圖像分析，則很可能需要更一般的資料處理模型。這些類型的處理通常是特別針對特定應用的（例如機器學習的特徵工程，機器翻譯的自然語言模型，欺詐預測的風險評估函數），因此它們不可避免地需要編寫代碼，而不僅僅是查詢。

   MapReduce使工程師能夠輕鬆地在大型資料集上運行自己的代碼。如果你有HDFS和MapReduce，那麼你**可以**在它之上建立一個SQL查詢執行引擎，事實上這正是Hive專案所做的【31】。但是，你也可以編寫許多其他形式的批次處理，這些批次處理不必非要用SQL查詢表示。

   隨後，人們發現MapReduce對於某些類型的處理而言局限性很大，表現很差，因此在Hadoop之上其他各種處理模型也被開發出來（我們將在“[MapReduce之後](#後MapReduce時代)”中看到其中一些）。有兩種處理模型，SQL和MapReduce，還不夠，需要更多不同的模型！而且由於Hadoop平臺的開放性，實施一整套方法是可行的，而這在單體MPP資料庫的範疇內是不可能的【58】。

   至關重要的是，這些不同的處理模型都可以在共用的單個機器集群上運行，所有這些機器都可以訪問分散式檔案系統上的相同檔。在Hadoop方法中，不需要將資料導入到幾個不同的專用系統中進行不同類型的處理：系統足夠靈活，可以支援同一個群集內不同的工作負載。不需要移動資料，使得從資料中挖掘價值變得容易得多，也使採用新的處理模型容易的多。

   Hadoop生態系統包括隨機訪問的OLTP資料庫，如HBase（參閱“[SSTables和LSM樹](ch3.md#SSTables和LSM樹)”）和MPP風格的分析型資料庫，如Impala 【41】。 HBase與Impala都不使用MapReduce，但都使用HDFS進行存儲。它們是迥異的資料訪問與處理方法，但是它們可以共存，並被集成到同一個系統中。

#### 針對頻繁故障設計

   當比較MapReduce和MPP資料庫時，兩種不同的設計思路出現了：處理故障和使用記憶體與磁片的方式。與線上系統相比，批次處理對故障不太敏感，因為就算失敗也不會立即影響到用戶，而且它們總是能再次運行。

   如果一個節點在執行查詢時崩潰，大多數MPP資料庫會中止整個查詢，並讓用戶重新提交查詢或自動重新運行它【3】。由於查詢通常最多運行幾秒鐘或幾分鐘，所以這種錯誤處理的方法是可以接受的，因為重試的代價不是太大。 MPP資料庫還傾向於在記憶體中保留盡可能多的資料（例如，使用散列連接）以避免從磁片讀取的開銷。

   另一方面，MapReduce可以容忍單個Map或Reduce任務的失敗，而不會影響作業的整體，通過以單個任務的細微性重試工作。它也會非常急切地將資料寫入磁片，一方面是為了容錯，另一部分是因為假設資料集太大而不能適應記憶體。

   MapReduce方式更適用於較大的作業：要處理如此之多的資料並運行很長時間的作業，以至於在此過程中很可能至少遇到一個任務故障。在這種情況下，由於單個任務失敗而重新運行整個作業將是非常浪費的。即使以單個任務的細微性進行恢復引入了使得無故障處理更慢的開銷，但如果任務失敗率足夠高，這仍然是一種合理的權衡。

   但是這些假設有多麼現實呢？在大多數集群中，機器故障確實會發生，但是它們不是很頻繁 —— 可能少到絕大多數作業都不會經歷機器故障。為了容錯，真的值得帶來這麼大的額外開銷嗎？

   要瞭解MapReduce節約使用記憶體和在任務的層次進行恢復的原因，瞭解最初設計MapReduce的環境是很有幫助的。 Google有著混用的資料中心，線上生產服務和離線批次處理作業在同樣機器上運行。每個任務都有一個通過容器強制執行的資源配給（CPU核心，RAM，磁碟空間等）。每個任務也具有優先順序，如果優先順序較高的任務需要更多的資源，則可以終止（搶佔）同一台機器上較低優先順序的任務以釋放資源。優先順序還決定了計算資源的定價：團隊必須為他們使用的資源付費，而優先順序更高的進程花費更多【59】。

   這種架構允許非生產（低優先順序）計算資源被**過量使用（overcommitted）**，因為系統知道必要時它可以回收資源。與分離生產和非生產任務的系統相比，過量使用資源可以更好地利用機器並提高效率。但由於MapReduce作業以低優先順序運行，它們隨時都有被搶佔的風險，因為優先順序較高的進程可能需要其資源。在高優先順序進程拿走所需資源後，批量作業能有效地“撿麵包屑”，利用剩下的任何計算資源。

   在穀歌，運行一個小時的MapReduce任務有大約有5％的風險被終止，為了給更高優先順序的進程挪地方。這一概率比硬體問題，機器重啟或其他原因的概率高了一個數量級【59】。按照這種搶佔率，如果一個作業有100個任務，每個任務運行10分鐘，那麼至少有一個任務在完成之前被終止的風險大於50％。

   這就是MapReduce被設計為容忍頻繁意外任務終止的原因：不是因為硬體很不可靠，而是因為任意終止進程的自由有利於提高計算集群中的資源利用率。

   在開源的集群調度器中，搶佔的使用較少。 YARN的CapacityScheduler支援搶佔，以平衡不同佇列的資源配置【58】，但在編寫本文時，YARN，Mesos或Kubernetes不支援通用優先順序搶佔【60】。在任務不經常被終止的環境中，MapReduce的這一設計決策就沒有多少意義了。在下一節中，我們將研究一些與MapReduce設計決策相異的替代方案。


## MapReduce之後

   雖然MapReduce在二十世紀二十年代後期變得非常流行，並受到大量的炒作，但它只是分散式系統的許多可能的程式設計模型之一。對於不同的資料量，資料結構和處理類型，其他工具可能更適合表示計算。

   不管如何，我們在這一章花了大把時間來討論MapReduce，因為它是一種有用的學習工具，它是分散式檔案系統的一種相當簡單明晰的抽象。在這裡，**簡單**意味著我們能理解它在做什麼，而不是意味著使用它很簡單。恰恰相反：使用原始的MapReduce API來實現複雜的處理工作實際上是非常困難和費力的 —— 例如，任意一種連接演算法都需要你從頭開始實現【37】。

   針對直接使用MapReduce的困難，在MapReduce上有很多高級程式設計模型（Pig，Hive，Cascading，Crunch）被創造出來，作為建立在MapReduce之上的抽象。如果你瞭解MapReduce的原理，那麼它們學起來相當簡單。而且它們的高級結構能顯著簡化許多常見批次處理任務的實現。

   但是，MapReduce執行模型本身也存在一些問題，這些問題並沒有通過增加另一個抽象層次而解決，而對於某些類型的處理，它表現得非常差勁。一方面，MapReduce非常穩健：你可以使用它在任務會頻繁終止的多租戶系統上處理幾乎任意大量級的資料，並且仍然可以完成工作（雖然速度很慢）。另一方面，對於某些類型的處理而言，其他工具有時會快上幾個數量級。

   在本章的其餘部分中，我們將介紹一些批次處理方法。在[第11章](ch11.md)我們將轉向流處理，它可以看作是加速批次處理的另一種方法。

### 物化中間狀態

   如前所述，每個MapReduce作業都獨立於其他任何作業。作業與世界其他地方的主要連接點是分散式檔案系統上的輸入和輸出目錄。如果希望一個作業的輸出成為第二個作業的輸入，則需要將第二個作業的輸入目錄配置為第一個作業輸出目錄，且外部工作流調度程式必須在第一個作業完成後再啟動第二個。

   如果第一個作業的輸出是要在組織內廣泛發佈的資料集，則這種配置是合理的。在這種情況下，你需要通過名稱引用它，並將其重用為多個不同作業的輸入（包括由其他團隊開發的作業）。將資料發佈到分散式檔案系統中眾所周知的位置能夠帶來**松耦合**，這樣作業就不需要知道是誰在提供輸入或誰在消費輸出（參閱“[邏輯與佈線相分離](#邏輯與佈線相分離)”）。

   但在很多情況下，你知道一個作業的輸出只能用作另一個作業的輸入，這些作業由同一個團隊維護。在這種情況下，分散式檔案系統上的檔只是簡單的**中間狀態（intermediate state）**：一種將資料從一個作業傳遞到下一個作業的方式。在一個用於構建推薦系統的，由50或100個MapReduce作業組成的複雜工作流中，存在著很多這樣的中間狀態【29】。

   將這個中間狀態寫入檔的過程稱為**物化（materialization）**。 （在“[聚合：資料立方體和物化視圖](ch2.md#聚合：資料立方體和物化視圖)”中已經在物化視圖的背景中遇到過這個術語。它意味著對某個操作的結果立即求值並寫出來，而不是在請求時按需計算）

   作為對照，本章開頭的日誌分析示例使用Unix管道將一個命令的輸出與另一個命令的輸入連接起來。管道並沒有完全物化中間狀態，而是只使用一個小的記憶體緩衝區，將輸出增量地**流（stream）**向輸入。

   與Unix管道相比，MapReduce完全物化中間狀態的方法存在不足之處：

- MapReduce作業只有在前驅作業（生成其輸入）中的所有任務都完成時才能啟動，而由Unix管道連接的進程會同時啟動，輸出一旦生成就會被消費。不同機器上的資料傾斜或負載不均意味著一個作業往往會有一些掉隊的任務，比其他任務要慢得多才能完成。必須等待至前驅作業的所有任務完成，拖慢了整個工作流程的執行。
- Mapper通常是多餘的：它們僅僅是讀取剛剛由Reducer寫入的同樣檔，為下一個階段的分區和排序做準備。在許多情況下，Mapper代碼可能是前驅Reducer的一部分：如果Reducer和Mapper的輸出有著相同的分區與排序方式，那麼Reducer就可以直接串在一起，而不用與Mapper相互交織。
- 將中間狀態存儲在分散式檔案系統中意味著這些檔被複製到多個節點，這些臨時資料這麼搞就比較過分了。

#### 資料流程引擎

   了解決MapReduce的這些問題，幾種用於分散式批次處理的新執行引擎被開發出來，其中最著名的是Spark 【61,62】，Tez 【63,64】和Flink 【65,66】。它們的設計方式有很多區別，但有一個共同點：把整個工作流作為單個作業來處理，而不是把它分解為獨立的子作業。

   由於它們將工作流顯式建模為 資料從幾個處理階段穿過，所以這些系統被稱為**資料流程引擎（dataflow engines）**。像MapReduce一樣，它們在一條線上通過反復調用使用者定義的函數來一次處理一條記錄，它們通過輸入分區來並行化載荷，它們通過網路將一個函數的輸出複製到另一個函數的輸入。

   與MapReduce不同，這些功能不需要嚴格扮演交織的Map與Reduce的角色，而是可以以更靈活的方式進行組合。我們稱這些函數為**運算元（operators）**，資料流程引擎提供了幾種不同的選項來將一個運算元的輸出連接到另一個運算元的輸入：

- 一種選項是對記錄按鍵重新分區並排序，就像在MapReduce的混洗階段一樣（參閱“[分散式執行MapReduce](#分散式執行MapReduce)”）。這種功能可以用於實現排序合併連接和分組，就像在MapReduce中一樣。
- 另一種可能是接受多個輸入，並以相同的方式進行分區，但跳過排序。當記錄的分區重要但順序無關緊要時，這省去了分區散列連接的工作，因為構建散清單還是會把順序隨機打亂。
- 對於廣播散列連接，可以將一個運算元的輸出，發送到連接運算元的所有分區。

這種類型的處理引擎是基於像Dryad 【67】和Nephele 【68】這樣的研究系統，與MapReduce模型相比，它有幾個優點：

- 排序等昂貴的工作只需要在實際需要的地方執行，而不是默認地在每個Map和Reduce階段之間出現。
- 沒有不必要的Map任務，因為Mapper所做的工作通常可以合併到前面的Reduce運算元中（因為Mapper不會更改資料集的分區）。
- 由於工作流中的所有連接和資料依賴都是顯式聲明的，因此調度程式能夠總覽全域，知道哪裡需要哪些資料，因而能夠利用局部性進行優化。例如，它可以嘗試將消費某些資料的任務放在與生成這些資料的任務相同的機器上，從而資料可以通過共用記憶體緩衝區傳輸，而不必通過網路複製。
- 通常，運算元間的中間狀態足以保存在記憶體中或寫入本地磁片，這比寫入HDFS需要更少的I/O（必須將其複製到多台機器，並將每個副本寫入磁片）。 MapReduce已經對Mapper的輸出做了這種優化，但資料流程引擎將這種思想推廣至所有的中間狀態。
- 運算元可以在輸入就緒後立即開始執行；後續階段無需等待前驅階段整個完成後再開始。
- 與MapReduce（為每個任務啟動一個新的JVM）相比，現有Java虛擬機器（JVM）進程可以重用來運行新運算元，從而減少啟動開銷。

你可以使用資料流程引擎執行與MapReduce工作流同樣的計算，而且由於此處所述的優化，通常執行速度要明顯快得多。既然運算元是Map和Reduce的泛化，那麼相同的處理代碼就可以在任一執行引擎上運行：Pig，Hive或Cascading中實現的工作流可以無需修改代碼，可以通過修改配置，簡單地從MapReduce切換到Tez或Spark【64】。

   Tez是一個相當薄的庫，它依賴於YARN shuffle服務來實現節點間資料的實際複製【58】，而Spark和Flink則是包含了獨立網路通信層，調度器，及用戶向API的大型框架。我們將簡要討論這些高級API。

#### 容錯

   完全物化中間狀態至分散式檔案系統的一個優點是，它具有持久性，這使得MapReduce中的容錯相當容易：如果一個任務失敗，它可以在另一台機器上重新啟動，並從檔案系統重新讀取相同的輸入。

   Spark，Flink和Tez避免將中間狀態寫入HDFS，因此它們採取了不同的方法來容錯：如果一台機器發生故障，並且該機器上的中間狀態丟失，則它會從其他仍然可用的資料重新計算（在可行的情況下是先前的中間狀態，要麼就只能是原始輸入資料，通常在HDFS上）。

   為了實現這種重新計算，框架必須跟蹤一個給定的資料是如何計算的 —— 使用了哪些輸入分區？應用了哪些運算元？ Spark使用**彈性分散式資料集（RDD）**的抽象來跟蹤資料的譜系【61】，而Flink對運算元狀態存檔，允許恢復運行在執行過程中遇到錯誤的運算元【66】。

   在重新計算資料時，重要的是要知道計算是否是**確定性的**：也就是說，給定相同的輸入資料，運算元是否始終產生相同的輸出？如果一些丟失的資料已經發送給下游運算元，這個問題就很重要。如果運算元重新開機，重新計算的資料與原有的丟失資料不一致，下游運算元很難解決新舊資料之間的矛盾。對於不確定性運算元來說，解決方案通常是殺死下游運算元，然後再重跑新資料。

   為了避免這種級聯故障，最好讓運算元具有確定性。但需要注意的是，非確定性行為很容易悄悄溜進來：例如，許多程式設計語言在反覆運算雜湊表的元素時不能對順序作出保證，許多概率和統計演算法顯式依賴於使用亂數，以及用到系統時鐘或外部資料來源，這些都是都不確定性的行為。為了能可靠地從故障中恢復，需要消除這種不確定性因素，例如使用固定的種子生成偽亂數。

   通過重算數據來從故障中恢復並不總是正確的答案：如果中間狀態資料要比來源資料小得多，或者如果計算量非常大，那麼將中間資料物化為檔可能要比重新計算廉價的多。

#### 關於物化的討論

   回到Unix的類比，我們看到，MapReduce就像是將每個命令的輸出寫入暫存檔案，而資料流程引擎看起來更像是Unix管道。尤其是Flink是基於管道執行的思想而建立的：也就是說，將運算元的輸出增量地傳遞給其他運算元，不待輸入完成便開始處理。

   排序運算元不可避免地需要消費全部的輸入後才能生成任何輸出，因為輸入中最後一條輸入記錄可能具有最小的鍵，因此需要作為第一條記錄輸出。因此，任何需要排序的運算元都需要至少暫時地累積狀態。但是工作流的許多其他部分可以以流水線方式執行。

   當作業完成時，它的輸出需要持續到某個地方，以便用戶可以找到並使用它—— 很可能它會再次寫入分散式檔案系統。因此，在使用資料流程引擎時，HDFS上的物化資料集通常仍是作業的輸入和最終輸出。和MapReduce一樣，輸入是不可變的，輸出被完全替換。比起MapReduce的改進是，你不用再自己去將中間狀態寫入檔案系統了。

### 圖與反覆運算處理

   在“[圖資料模型](ch2.md#圖資料模型)”中，我們討論了使用圖來建模資料，並使用圖查詢語言來遍歷圖中的邊與點。[第2章](ch2.md)的討論集中在OLTP風格的應用場景：快速執行查詢來查找少量符合特定條件的頂點。

   批次處理上下文中的圖也很有趣，其目標是在整個圖上執行某種離線處理或分析。這種需求經常出現在機器學習應用（如推薦引擎）或排序系統中。例如，最著名的圖形分析演算法之一是PageRank 【69】，它試圖根據連結到某個網頁的其他網頁來估計該網頁的流行度。它作為配方的一部分，用於確定網路搜尋引擎呈現結果的順序

> 像Spark，Flink和Tez這樣的資料流程引擎（參見“[中間狀態的物化](#中間狀態的物化)”）通常將運算元作為**有向無環圖（DAG）**的一部分安排在作業中。這與圖處理不一樣：在資料流程引擎中，**從一個運算元到另一個運算元的資料流程**被構造成一個圖，而資料本身通常由關聯式元組構成。在圖處理中，資料本身具有圖的形式。又一個不幸的命名混亂！

   許多圖演算法是通過一次遍歷一條邊來表示的，將一個頂點與近鄰的頂點連接起來，以傳播一些資訊，並不斷重複，直到滿足一些條件為止 —— 例如，直到沒有更多的邊要跟進，或直到一些指標收斂。我們在[圖2-6](img/fig2-6.png)中看到一個例子，它通過重複跟進標明地點歸屬關係的邊，生成了資料庫中北美包含的所有地點清單（這種演算法被稱為**閉包傳遞（transitive closure）**）。

   可以在分散式檔案系統中存儲圖（包含頂點和邊的列表的檔），但是這種“重複至完成”的想法不能用普通的MapReduce來表示，因為它只掃過一趟資料。這種演算法因此經常以**反覆運算**的風格實現：

1. 外部調度程式運行批次處理來計算演算法的一個步驟。
2. 當批次處理過程完成時，調度器檢查它是否完成（基於完成條件 —— 例如，沒有更多的邊要跟進，或者與上次反覆運算相比的變化低於某個閾值）。
3. 如果尚未完成，則調度程式返回到步驟1並運行另一輪批次處理。

這種方法是有效的，但是用MapReduce實現它往往非常低效，因為MapReduce沒有考慮演算法的反覆運算性質：它總是讀取整個輸入資料集並產生一個全新的輸出資料集，即使與上次反覆運算相比，改變的僅僅是圖中的一小部分。

#### Pregel處理模型

   針對圖批次處理的優化 —— **批量同步並行（BSP）**計算模型【70】已經開始流行起來。其中，Apache Giraph 【37】，Spark的GraphX API和Flink的Gelly API 【71】實現了它。它也被稱為**Pregel**模型，因為Google的Pregel論文推廣了這種處理圖的方法【72】。

   回想一下在MapReduce中，Mapper在概念上向Reducer的特定調用“發送消息”，因為框架將所有具有相同鍵的Mapper輸出集中在一起。 Pregel背後有一個類似的想法：一個頂點可以向另一個頂點“發送消息”，通常這些消息是沿著圖的邊發送的。

   在每次反覆運算中，為每個頂點調用一個函數，將所有發送給它的消息傳遞給它 —— 就像調用Reducer一樣。與MapReduce的不同之處在於，在Pregel模型中，頂點在一次反覆運算到下一次反覆運算的過程中會記住它的狀態，所以這個函數只需要處理新的傳入消息。如果圖的某個部分沒有被發送消息，那裡就不需要做任何工作。

   這與Actor模型有些相似（參閱“[分散式的Actor框架](ch4.md#分散式的Actor框架)”），除了頂點狀態和頂點之間的消息具有容錯性和耐久性，且通信以固定的方式進行：在每次反覆運算中，框架遞送上次反覆運算中發送的所有消息。Actor通常沒有這樣的時間保證。

#### 容錯

   頂點只能通過消息傳遞進行通信（而不是直接相互查詢）的事實有助於提高Pregel作業的性能，因為消息可以成批次處理，且等待通信的次數也減少了。唯一的等待是在反覆運算之間：由於Pregel模型保證所有在一輪反覆運算中發送的消息都在下輪反覆運算中送達，所以在下一輪反覆運算開始前，先前的反覆運算必須完全完成，而所有的消息必須在網路上完成複製。

   即使底層網路可能丟失，重複或任意延遲消息（參閱“[不可靠的網路](ch8.md#不可靠的網路)”），Pregel的實現能保證在後續反覆運算中消息在其目標頂點恰好處理一次。像MapReduce一樣，框架能從故障中透明地恢復，以簡化在Pregel上實現演算法的程式設計模型。

   這種容錯是通過在反覆運算結束時，定期存檔所有頂點的狀態來實現的，即將其全部狀態寫入持久化存儲。如果某個節點發生故障並且其記憶體中的狀態丟失，則最簡單的解決方法是將整個圖計算回滾到上一個存檔點，然後重啟計算。如果演算法是確定性的，且消息記錄在日誌中，那麼也可以選擇性地只恢復丟失的分區（就像之前討論過的資料流程引擎）【72】。

#### 並存執行

   頂點不需要知道它在哪台物理機器上執行；當它向其他頂點發送消息時，它只是簡單地將消息發往某個頂點ID。圖的分區取決於框架 —— 即，確定哪個頂點運行在哪台機器上，以及如何通過網路路由消息，以便它們到達正確的地方。

   由於程式設計模型一次僅處理一個頂點（有時稱為“像頂點一樣思考”），所以框架可以以任意方式對圖分區。理想情況下如果頂點需要進行大量的通信，那麼它們最好能被分區到同一台機器上。然而找到這樣一種優化的分區方法是很困難的 —— 在實踐中，圖經常按照任意分配的頂點ID分區，而不會嘗試將相關的頂點分組在一起。

   因此，圖演算法通常會有很多跨機器通信的額外開銷，而中間狀態（節點之間發送的消息）往往比原始圖大。通過網路發送消息的開銷會顯著拖慢分散式圖演算法的速度。

   出於這個原因，如果你的圖可以放入一台電腦的記憶體中，那麼單機（甚至可能是單執行緒）演算法很可能會超越分散式批次處理【73,74】。圖比記憶體大也沒關係，只要能放入單台電腦的磁片，使用GraphChi等框架進行單機處理是就一個可行的選擇【75】。如果圖太大，不適合單機處理，那麼像Pregel這樣的分散式方法是不可避免的。高效的並行圖演算法是一個進行中的研究領域【76】。


### 高級API和語言

   自MapReduce開始流行的這幾年以來，分散式批次處理的執行引擎已經很成熟了。到目前為止，基礎設施已經足夠強大，能夠存儲和處理超過10,000台機器群集上的數PB的數據。由於在這種規模下物理執行批次處理的問題已經被認為或多或少解決了，所以關注點已經轉向其他領域：改進程式設計模型，提高處理效率，擴大這些技術可以解決的問題集。

   如前所述，Hive，Pig，Cascading和Crunch等高階語言和API變得越來越流行，因為手寫MapReduce作業實在是個苦力活。隨著Tez的出現，這些高階語言還有一個額外好處，可以遷移到新的資料流程執行引擎，而無需重寫作業代碼。 Spark和Flink也有它們自己的高級資料流程API，通常是從FlumeJava中獲取的靈感【34】。

   這些資料流程API通常使用關聯式構建塊來表達一個計算：按某個欄位連接資料集；按鍵對元組做分組；按某些條件過濾；並通過計數求和或其他函數來聚合元組。在內部，這些操作是使用本章前面討論過的各種連接和分組演算法來實現的。

   除了少寫代碼的明顯優勢之外，這些高級介面還支援互動式用法，在這種互動式使用中，你可以在Shell中增量式編寫分析代碼，頻繁運行來觀察它做了什麼。這種開發風格在探索資料集和試驗處理方法時非常有用。這也讓人聯想到Unix哲學，我們在“[Unix哲學](#Unix哲學)”中討論過這個問題。

   此外，這些高級介面不僅提高了人類的工作效率，也提高了機器層面的作業執行效率。

#### 向聲明式查詢語言的轉變

   與硬寫執行連接的代碼相比，指定連接關係運算元的優點是，框架可以分析連接輸入的屬性，並自動決定哪種上述連接演算法最適合當前任務。 Hive，Spark和Flink都有基於代價的查詢最佳化工具可以做到這一點，甚至可以改變連接順序，最小化中間狀態的數量【66,77,78,79】。

   連接演算法的選擇可以對批次處理作業的性能產生巨大影響，而無需理解和記住本章中討論的各種連接演算法。如果連接是以**聲明式（declarative）**的方式指定的，那這就這是可行的：應用只是簡單地說明哪些連接是必需的，查詢最佳化工具決定如何最好地執行連接。我們以前在“[資料查詢語言](ch2.md#資料查詢語言)”中見過這個想法。

   但MapReduce及其後繼者資料流程在其他方面，與SQL的完全聲明式查詢模型有很大區別。 MapReduce是圍繞著回呼函數的概念建立的：對於每條記錄或者一組記錄，調用一個使用者定義的函數（Mapper或Reducer），並且該函數可以自由地調用任意代碼來決定輸出什麼。這種方法的優點是可以基於大量已有庫的生態系統創作：解析，自然語言分析，圖像分析以及運行數值演算法或統計演算法等。

   自由運行任意代碼，長期以來都是傳統MapReduce批次處理系統與MPP資料庫的區別所在（參見“[比較Hadoop和分散式資料庫](#比較Hadoop和分散式資料庫)”一節）。雖然資料庫具有編寫使用者定義函數的功能，但是它們通常使用起來很麻煩，而且與大多數程式設計語言中廣泛使用的套裝程式管理器和依賴管理系統相容不佳（例如Java的Maven， Javascript的npm，以及Ruby的gems）。

   然而資料流程引擎已經發現，支援除連接之外的更多**聲明式特性**還有其他的優勢。例如，如果一個回呼函數隻包含一個簡單的過濾條件，或者只是從一條記錄中選擇了一些欄位，那麼在為每條記錄調用函數時會有相當大的額外CPU開銷。如果以聲明方式表示這些簡單的過濾和映射操作，那麼查詢最佳化工具可以利用面向列的存儲佈局（參閱“[列存儲](ch3.md#列存儲)”），只從磁片讀取所需的列。 Hive，Spark DataFrames和Impala還使用了向量化執行（參閱“[記憶體頻寬和向量處理](ch3.md#記憶體頻寬和向量處理)”）：在對CPU緩存友好的內部迴圈中反覆運算資料，避免函式呼叫。Spark生成JVM位元組碼【79】，Impala使用LLVM為這些內部迴圈生成本機代碼【41】。

   通過在高級API中引入聲明式的部分，並使查詢最佳化工具可以在執行期間利用這些來做優化，批次處理框架看起來越來越像MPP資料庫了（並且能實現可與之媲美的性能）。同時，通過擁有運行任意代碼和以任意格式讀取資料的可擴展性，它們保持了靈活性的優勢。

#### 專業化的不同領域

   儘管能夠運行任意代碼的可擴展性是很有用的，但是也有很多常見的例子，不斷重複著標準的處理模式。因而這些模式值得擁有自己的可重用通用構建模組實現，傳統上，MPP資料庫滿足了商業智慧分析和業務報表的需求，但這只是許多使用批次處理的領域之一。

   另一個越來越重要的領域是統計和數值演算法，它們是機器學習應用所需要的（例如分類器和推薦系統）。可重用的實現正在出現：例如，Mahout在MapReduce，Spark和Flink之上實現了用於機器學習的各種演算法，而MADlib在關聯式MPP資料庫（Apache HAWQ）中實現了類似的功能【54】。

   空間演算法也是有用的，例如**最近鄰搜索（k-nearest neghbors, kNN）**【80】，它在一些多維空間中搜索與給定項最近的項目 —— 這是一種相似性搜索。近似搜索對於基因組分析演算法也很重要，它們需要找到相似但不相同的字串【81】。

   批次處理引擎正被用於分散式執行日益廣泛的各領域演算法。隨著批次處理系統獲得各種內置功能以及高級聲明式運算元，且隨著MPP資料庫變得更加靈活和易於程式設計，兩者開始看起來相似了：最終，它們都只是存儲和處理資料的系統。


## 本章小結

   在本章中，我們探索了批次處理的主題。我們首先看到了諸如awk，grep和sort之類的Unix工具，然後我們看到了這些工具的設計理念是如何應用到MapReduce和更近的資料流程引擎中的。一些設計原則包括：輸入是不可變的，輸出是為了作為另一個（仍未知的）程式的輸入，而複雜的問題是通過編寫“做好一件事”的小工具來解決的。

   在Unix世界中，允許程式與程式組合的統一介面是檔與管道；在MapReduce中，該介面是一個分散式檔案系統。我們看到資料流程引擎添加了自己的管道式資料傳輸機制，以避免將中間狀態物化至分散式檔案系統，但作業的初始輸入和最終輸出通常仍是HDFS。

   分散式批次處理框架需要解決的兩個主要問題是：

***分區***

   在MapReduce中，Mapper根據輸入檔塊進行分區。Mapper的輸出被重新分區，排序，併合並到可配置數量的Reducer分區中。這一過程的目的是把所有的**相關**資料（例如帶有相同鍵的所有記錄）都放在同一個地方。

   後MapReduce時代的資料流程引擎若非必要會儘量避免排序，但它們也採取了大致類似的分區方法。

***容錯***

   MapReduce經常寫入磁片，這使得從單個失敗的任務恢復很輕鬆，無需重新啟動整個作業，但在無故障的情況下減慢了執行速度。資料流程引擎更多地將中間狀態保存在記憶體中，更少地物化中間狀態，這意味著如果節點發生故障，則需要重算更多的資料。確定性運算元減少了需要重算的資料量。


   我們討論了幾種MapReduce的連接演算法，其中大多數也在MPP資料庫和資料流程引擎內部使用。它們也很好地演示了分區演算法是如何工作的：

***排序合併連接***

   每個參與連接的輸入都通過一個提取連接鍵的Mapper。通過分區，排序和合併，具有相同鍵的所有記錄最終都會進入相同的Reducer調用。這個函數能輸出連接好的記錄。

***廣播散列連接***

   兩個連接輸入之一很小，所以它並沒有分區，而且能被完全載入進一個雜湊表中。因此，你可以為連接輸入大端的每個分區啟動一個Mapper，將輸入小端的散列表載入到每個Mapper中，然後掃描大端，一次一條記錄，並為每條記錄查詢散清單。

***分區散列連接***

   如果兩個連接輸入以相同的方式分區（使用相同的鍵，相同的散列函數和相同數量的分區），則可以獨立地對每個分區應用散列表方法。

   分散式批次處理引擎有一個刻意限制的程式設計模型：回呼函數（比如Mapper和Reducer）被假定是無狀態的，而且除了指定的輸出外，必須沒有任何外部可見的副作用。這一限制允許框架在其抽象下隱藏一些困難的分散式系統問題：當遇到崩潰和網路問題時，任務可以安全地重試，任何失敗任務的輸出都被丟棄。如果某個分區的多個任務成功，則其中只有一個能使其輸出實際可見。

   得益於這個框架，你在批次處理作業中的代碼無需操心實現容錯機制：框架可以保證作業的最終輸出與沒有發生錯誤的情況相同，也許不得不重試各種任務。線上服務處理使用者請求，並將寫入資料庫作為處理請求的副作用，比起線上服務，批次處理提供的這種可靠性語義要強得多。

   批次處理作業的顯著特點是，它讀取一些輸入資料並產生一些輸出資料，但不修改輸入—— 換句話說，輸出是從輸入衍生出的。最關鍵的是，輸入資料是**有界的（bounded）**：它有一個已知的，固定的大小（例如，它包含一些時間點的日誌檔或資料庫內容的快照）。因為它是有界的，一個作業知道自己什麼時候完成了整個輸入的讀取，所以一個工作在做完後，最終總是會完成的。

   在下一章中，我們將轉向流處理，其中的輸入是**無界的（unbounded）** —— 也就是說，你還有活兒要幹，然而它的輸入是永無止境的資料流程。在這種情況下，作業永無完成之日。因為在任何時候都可能有更多的工作湧入。我們將看到，在某些方面上，流處理和批次處理是相似的。但是關於無盡資料流程的假設也對我們構建系統的方式產生了很多改變。




## 參考文獻

1.  Jeffrey Dean and Sanjay Ghemawat: “[MapReduce: Simplified Data Processing on Large Clusters](http://research.google.com/archive/mapreduce.html),” at *6th USENIX Symposium on Operating System Design and Implementation* (OSDI), December 2004.

1.  Joel Spolsky: “[The Perils of JavaSchools](http://www.joelonsoftware.com/articles/ThePerilsofJavaSchools.html),” *joelonsoftware.com*, December 25, 2005.

1.  Shivnath Babu and Herodotos Herodotou: “[Massively Parallel Databases and MapReduce Systems](http://research.microsoft.com/pubs/206464/db-mr-survey-final.pdf),” *Foundations and Trends in Databases*, volume 5, number 1, pages 1–104, November 2013. [doi:10.1561/1900000036](http://dx.doi.org/10.1561/1900000036)

1.  David J. DeWitt and Michael Stonebraker: “[MapReduce: A Major Step Backwards](https://homes.cs.washington.edu/~billhowe/mapreduce_a_major_step_backwards.html),” originally published at *databasecolumn.vertica.com*, January 17, 2008.

1.  Henry Robinson: “[The Elephant Was a Trojan Horse: On the Death of Map-Reduce at Google](http://the-paper-trail.org/blog/the-elephant-was-a-trojan-horse-on-the-death-of-map-reduce-at-google/),”
    *the-paper-trail.org*, June 25, 2014.

1.  “[The Hollerith Machine](https://www.census.gov/history/www/innovations/technology/the_hollerith_tabulator.html),” United States Census Bureau, *census.gov*.

1.  “[IBM 82, 83, and 84 Sorters Reference Manual](http://www.textfiles.com/bitsavers/pdf/ibm/punchedCard/Sorter/A24-1034-1_82-83-84_sorters.pdf),” Edition A24-1034-1, International Business
    Machines Corporation, July 1962.

1.  Adam Drake: “[Command-Line Tools Can Be 235x Faster than Your Hadoop Cluster](http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html),” *aadrake.com*, January 25, 2014.

1.  “[GNU Coreutils 8.23 Documentation](http://www.gnu.org/software/coreutils/manual/html_node/index.html),” Free Software Foundation, Inc., 2014.

1.  Martin Kleppmann: “[Kafka, Samza, and the Unix Philosophy of Distributed Data](http://martin.kleppmann.com/2015/08/05/kafka-samza-unix-philosophy-distributed-data.html),” *martin.kleppmann.com*, August 5, 2015.

1.  Doug McIlroy:[Internal Bell Labs memo](http://cm.bell-labs.com/cm/cs/who/dmr/mdmpipe.pdf), October 1964. Cited in: Dennis M. Richie: “[Advice from Doug McIlroy](https://www.bell-labs.com/usr/dmr/www/mdmpipe.html),” *cm.bell-labs.com*.

1.  M. D. McIlroy, E. N. Pinson, and B. A. Tague: “[UNIX Time-Sharing System: Foreword](https://archive.org/details/bstj57-6-1899),” *The Bell System Technical Journal*, volume 57, number 6, pages 1899–1904, July 1978.

1.  Eric S. Raymond: <a href="http://www.catb.org/~esr/writings/taoup/html/">*The Art of UNIX Programming*</a>. Addison-Wesley, 2003. ISBN: 978-0-13-142901-7

1.  Ronald Duncan: “[Text File Formats – ASCII Delimited Text – Not CSV or TAB Delimited Text](https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/),”
    *ronaldduncan.wordpress.com*, October 31, 2009.

1.  Alan Kay: “[Is 'Software Engineering' an Oxymoron?](http://tinlizzie.org/~takashi/IsSoftwareEngineeringAnOxymoron.pdf),” *tinlizzie.org*.

1.  Martin Fowler: “[InversionOfControl](http://martinfowler.com/bliki/InversionOfControl.html),” *martinfowler.com*, June 26, 2005.

1.  Daniel J. Bernstein: “[Two File Descriptors for Sockets](http://cr.yp.to/tcpip/twofd.html),” *cr.yp.to*.

1.  Rob Pike and Dennis M. Ritchie: “[The Styx Architecture for Distributed Systems](http://doc.cat-v.org/inferno/4th_edition/styx),” *Bell Labs Technical Journal*, volume 4, number 2, pages 146–152, April 1999.

1.  Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung: “[The Google File System](http://research.google.com/archive/gfs-sosp2003.pdf),” at *19th ACM Symposium on Operating Systems Principles* (SOSP), October 2003. [doi:10.1145/945445.945450](http://dx.doi.org/10.1145/945445.945450)

1.  Michael Ovsiannikov, Silvius Rus, Damian Reeves, et al.: “[The Quantcast File System](http://db.disi.unitn.eu/pages/VLDBProgram/pdf/industry/p808-ovsiannikov.pdf),” *Proceedings of the VLDB Endowment*, volume 6, number 11, pages 1092–1101, August 2013. [doi:10.14778/2536222.2536234](http://dx.doi.org/10.14778/2536222.2536234)

1.  “[OpenStack Swift 2.6.1 Developer Documentation](http://docs.openstack.org/developer/swift/),” OpenStack Foundation, *docs.openstack.org*, March 2016.

1.  Zhe Zhang, Andrew Wang, Kai Zheng, et al.: “[Introduction to HDFS Erasure Coding in Apache Hadoop](http://blog.cloudera.com/blog/2015/09/introduction-to-hdfs-erasure-coding-in-apache-hadoop/),” *blog.cloudera.com*, September 23, 2015.

1.  Peter Cnudde: “[Hadoop Turns 10](http://yahoohadoop.tumblr.com/post/138739227316/hadoop-turns-10),” *yahoohadoop.tumblr.com*, February 5, 2016.

1.  Eric Baldeschwieler: “[Thinking About the HDFS vs. Other Storage Technologies](http://hortonworks.com/blog/thinking-about-the-hdfs-vs-other-storage-technologies/),” *hortonworks.com*, July 25, 2012.

1.  Brendan Gregg: “[Manta: Unix Meets Map Reduce](http://dtrace.org/blogs/brendan/2013/06/25/manta-unix-meets-map-reduce/),” *dtrace.org*, June 25, 2013.

1.  Tom White: *Hadoop: The Definitive Guide*, 4th edition. O'Reilly Media, 2015. ISBN: 978-1-491-90163-2

1.  Jim N. Gray: “[Distributed Computing Economics](http://arxiv.org/pdf/cs/0403019.pdf),” Microsoft Research Tech Report MSR-TR-2003-24, March 2003.

1.  Márton Trencséni: “[Luigi vs Airflow vs Pinball](http://bytepawn.com/luigi-airflow-pinball.html),” *bytepawn.com*, February 6, 2016.

1.  Roshan Sumbaly, Jay Kreps, and Sam Shah: “[The 'Big Data' Ecosystem at LinkedIn](http://www.slideshare.net/s_shah/the-big-data-ecosystem-at-linkedin-23512853),” at *ACM International Conference on Management of Data (SIGMOD)*, July 2013. [doi:10.1145/2463676.2463707](http://dx.doi.org/10.1145/2463676.2463707)

1.  Alan F. Gates, Olga Natkovich, Shubham Chopra, et al.: “[Building a High-Level Dataflow System on Top of Map-Reduce: The Pig Experience](http://www.vldb.org/pvldb/2/vldb09-1074.pdf),” at *35th International Conference on Very Large Data Bases* (VLDB), August 2009.

1.  Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, et al.: “[Hive – A Petabyte Scale Data Warehouse Using Hadoop](http://i.stanford.edu/~ragho/hive-icde2010.pdf),” at *26th IEEE International Conference on Data Engineering* (ICDE), March 2010. [doi:10.1109/ICDE.2010.5447738](http://dx.doi.org/10.1109/ICDE.2010.5447738)

1.  “[Cascading 3.0 User Guide](http://docs.cascading.org/cascading/3.0/userguide/),” Concurrent, Inc., *docs.cascading.org*, January 2016.

1.  “[Apache Crunch User Guide](https://crunch.apache.org/user-guide.html),” Apache Software Foundation, *crunch.apache.org*.

1.  Craig Chambers, Ashish Raniwala, Frances Perry, et al.: “[FlumeJava: Easy, Efficient Data-Parallel Pipelines](https://research.google.com/pubs/archive/35650.pdf),” at *31st ACM SIGPLAN Conference on Programming Language Design and Implementation* (PLDI), June 2010. [doi:10.1145/1806596.1806638](http://dx.doi.org/10.1145/1806596.1806638)

1.  Jay Kreps: “[Why Local State is a Fundamental Primitive in Stream Processing](https://www.oreilly.com/ideas/why-local-state-is-a-fundamental-primitive-in-stream-processing),” *oreilly.com*, July 31, 2014.

1.  Martin Kleppmann: “[Rethinking Caching in Web Apps](http://martin.kleppmann.com/2012/10/01/rethinking-caching-in-web-apps.html),” *martin.kleppmann.com*, October 1, 2012.

1.  Mark Grover, Ted Malaska, Jonathan Seidman, and Gwen Shapira: *[Hadoop Application Architectures](http://shop.oreilly.com/product/0636920033196.do)*. O'Reilly Media, 2015. ISBN: 978-1-491-90004-8

1.  Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.: “[Challenges to Adopting Stronger Consistency at Scale](https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-ajoux.pdf),” at *15th USENIX Workshop on Hot Topics in Operating Systems* (HotOS), May 2015.

1.  Sriranjan Manjunath: “[Skewed Join](https://wiki.apache.org/pig/PigSkewedJoinSpec),” *wiki.apache.org*, 2009.

1.  David J. DeWitt, Jeffrey F. Naughton, Donovan A.Schneider, and S. Seshadri: “[Practical Skew Handling in Parallel Joins](http://www.vldb.org/conf/1992/P027.PDF),” at *18th International Conference on Very Large Data Bases* (VLDB), August 1992.

1.  Marcel Kornacker, Alexander Behm, Victor Bittorf, et al.: “[Impala: A Modern, Open-Source SQL Engine for Hadoop](http://pandis.net/resources/cidr15impala.pdf),” at *7th Biennial Conference on Innovative Data Systems Research* (CIDR), January 2015.

1.  Matthieu Monsch: “[Open-Sourcing PalDB, a Lightweight Companion for Storing Side Data](https://engineering.linkedin.com/blog/2015/10/open-sourcing-paldb--a-lightweight-companion-for-storing-side-da),” *engineering.linkedin.com*, October 26, 2015.

1.  Daniel Peng and Frank Dabek: “[Large-Scale Incremental Processing Using Distributed Transactions and Notifications](https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf),” at *9th USENIX conference on Operating Systems Design and Implementation* (OSDI), October 2010.

1.  “["Cloudera Search User Guide,"](http://www.cloudera.com/documentation/cdh/5-1-x/Search/Cloudera-Search-User-Guide/Cloudera-Search-User-Guide.html) Cloudera, Inc., September 2015.

1.  Lili Wu, Sam Shah, Sean Choi, et al.:
    “[The Browsemaps: Collaborative Filtering at LinkedIn](http://ls13-www.cs.uni-dortmund.de/homepage/rsweb2014/papers/rsweb2014_submission_3.pdf),” at *6th Workshop on Recommender Systems and the Social Web* (RSWeb), October 2014.

1.  Roshan Sumbaly, Jay Kreps, Lei Gao, et al.: “[Serving Large-Scale Batch Computed Data with Project Voldemort](http://static.usenix.org/events/fast12/tech/full_papers/Sumbaly.pdf),” at *10th USENIX Conference on File and Storage Technologies* (FAST), February 2012.

1.  Varun Sharma: “[Open-Sourcing Terrapin: A Serving System for Batch Generated Data](https://engineering.pinterest.com/blog/open-sourcing-terrapin-serving-system-batch-generated-data-0),”  *engineering.pinterest.com*, September 14, 2015.

1.  Nathan Marz: “[ElephantDB](http://www.slideshare.net/nathanmarz/elephantdb),” *slideshare.net*, May 30, 2011.

1.  Jean-Daniel (JD) Cryans: “[How-to: Use HBase Bulk Loading, and Why](http://blog.cloudera.com/blog/2013/09/how-to-use-hbase-bulk-loading-and-why/),” *blog.cloudera.com*, September 27, 2013.

1.  Nathan Marz:  “[How to Beat the CAP   Theorem](http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html),” *nathanmarz.com*, October 13, 2011.

1.  Molly Bartlett Dishman and Martin Fowler:  “[Agile   Architecture](http://conferences.oreilly.com/software-architecture/sa2015/public/schedule/detail/40388),” at *O'Reilly Software Architecture Conference*, March 2015.

1.  David J. DeWitt and Jim N. Gray: “[Parallel Database Systems: The Future of High Performance Database Systems](http://www.cs.cmu.edu/~pavlo/courses/fall2013/static/papers/dewittgray92.pdf),” *Communications of the ACM*, volume 35, number 6, pages 85–98, June 1992. [doi:10.1145/129888.129894](http://dx.doi.org/10.1145/129888.129894)

1.  Jay Kreps: “[But the multi-tenancy thing is actually really really hard](https://twitter.com/jaykreps/status/528235702480142336),” tweetstorm, *twitter.com*, October 31, 2014.

1.  Jeffrey Cohen, Brian Dolan, Mark Dunlap, et al.: “[MAD Skills: New Analysis Practices for Big Data](http://www.vldb.org/pvldb/2/vldb09-219.pdf),” *Proceedings of the VLDB Endowment*, volume 2, number 2, pages 1481–1492, August 2009. [doi:10.14778/1687553.1687576](http://dx.doi.org/10.14778/1687553.1687576)

1.  Ignacio Terrizzano, Peter Schwarz, Mary Roth, and John E. Colino: “[Data Wrangling: The Challenging Journey from the Wild to the Lake](http://cidrdb.org/cidr2015/Papers/CIDR15_Paper2.pdf),” at *7th Biennial Conference on Innovative Data Systems Research* (CIDR), January 2015.

1.  Paige Roberts: “[To Schema on Read or to Schema on Write, That Is the Hadoop Data Lake Question](http://adaptivesystemsinc.com/blog/to-schema-on-read-or-to-schema-on-write-that-is-the-hadoop-data-lake-question/),” *adaptivesystemsinc.com*, July 2, 2015.

1.  Bobby Johnson and Joseph Adler: “[The Sushi Principle: Raw Data Is Better](https://vimeo.com/123985284),” at *Strata+Hadoop World*, February 2015.

1.  Vinod Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, et al.: “[Apache Hadoop YARN: Yet Another Resource Negotiator](http://www.socc2013.org/home/program/a5-vavilapalli.pdf),” at *4th ACM Symposium on Cloud Computing* (SoCC), October 2013. [doi:10.1145/2523616.2523633](http://dx.doi.org/10.1145/2523616.2523633)

1.  Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, et al.: “[Large-Scale Cluster Management at Google with Borg](http://research.google.com/pubs/pub43438.html),” at *10th European Conference on Computer Systems* (EuroSys), April 2015. [doi:10.1145/2741948.2741964](http://dx.doi.org/10.1145/2741948.2741964)

1.  Malte Schwarzkopf: “[The Evolution of Cluster Scheduler Architectures](http://www.firmament.io/blog/scheduler-architectures.html),” *firmament.io*, March 9, 2016.

1.  Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, et al.: “[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf),” at *9th USENIX Symposium on Networked Systems Design and Implementation* (NSDI), April 2012.

1.  Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia: *Learning Spark*. O'Reilly Media, 2015. ISBN: 978-1-449-35904-1

1.  Bikas Saha and Hitesh Shah: “[Apache Tez: Accelerating Hadoop Query Processing](http://www.slideshare.net/Hadoop_Summit/w-1205phall1saha),” at *Hadoop Summit*, June 2014.

1.  Bikas Saha, Hitesh Shah, Siddharth Seth, et al.: “[Apache Tez: A Unifying Framework for Modeling and Building Data Processing Applications](http://home.cse.ust.hk/~weiwa/teaching/Fall15-COMP6611B/reading_list/Tez.pdf),” at *ACM International Conference on Management of Data* (SIGMOD), June 2015. [doi:10.1145/2723372.2742790](http://dx.doi.org/10.1145/2723372.2742790)

1.  Kostas Tzoumas: “[Apache Flink: API, Runtime, and Project Roadmap](http://www.slideshare.net/KostasTzoumas/apache-flink-api-runtime-and-project-roadmap),” *slideshare.net*, January 14, 2015.

1.  Alexander Alexandrov, Rico Bergmann, Stephan Ewen, et al.: “[The Stratosphere Platform for Big Data Analytics](https://ssc.io/pdf/2014-VLDBJ_Stratosphere_Overview.pdf),” *The VLDB Journal*, volume 23, number 6, pages 939–964, May 2014. [doi:10.1007/s00778-014-0357-y](http://dx.doi.org/10.1007/s00778-014-0357-y)

1.  Michael Isard, Mihai Budiu, Yuan Yu, et al.: “[Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks](http://research.microsoft.com/en-us/projects/dryad/eurosys07.pdf),” at *European Conference on Computer Systems* (EuroSys), March 2007. [doi:10.1145/1272996.1273005](http://dx.doi.org/10.1145/1272996.1273005)

1.  Daniel Warneke and Odej Kao: “[Nephele: Efficient Parallel Data Processing in the Cloud](https://stratosphere2.dima.tu-berlin.de/assets/papers/Nephele_09.pdf),” at *2nd Workshop on Many-Task Computing on Grids and Supercomputers* (MTAGS), November 2009. [doi:10.1145/1646468.1646476](http://dx.doi.org/10.1145/1646468.1646476)

1.  Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd: ["The PageRank"](http://ilpubs.stanford.edu:8090/422/)

1.  Leslie G. Valiant: “[A Bridging Model for Parallel Computation](http://dl.acm.org/citation.cfm?id=79181),” *Communications of the ACM*, volume 33, number 8, pages 103–111, August 1990. [doi:10.1145/79173.79181](http://dx.doi.org/10.1145/79173.79181)

1.  Stephan Ewen, Kostas Tzoumas, Moritz Kaufmann, and Volker Markl: “[Spinning Fast Iterative Data Flows](http://vldb.org/pvldb/vol5/p1268_stephanewen_vldb2012.pdf),” *Proceedings of the VLDB Endowment*, volume 5, number 11, pages 1268-1279, July 2012. [doi:10.14778/2350229.2350245](http://dx.doi.org/10.14778/2350229.2350245)

1.  Grzegorz Malewicz, Matthew H.Austern, Aart J. C. Bik, et al.: “[Pregel: A System for Large-Scale Graph Processing](https://kowshik.github.io/JPregel/pregel_paper.pdf),” at *ACM International Conference on Management of Data* (SIGMOD), June 2010. [doi:10.1145/1807167.1807184](http://dx.doi.org/10.1145/1807167.1807184)

1.  Frank McSherry, Michael Isard, and Derek G. Murray: “[Scalability! But at What COST?](http://www.frankmcsherry.org/assets/COST.pdf),” at *15th USENIX Workshop on Hot Topics in Operating Systems* (HotOS), May 2015.

1.  Ionel Gog, Malte Schwarzkopf, Natacha Crooks, et al.: “[Musketeer: All for One, One for All in Data Processing Systems](http://www.cl.cam.ac.uk/research/srg/netos/camsas/pubs/eurosys15-musketeer.pdf),” at *10th European Conference on Computer Systems* (EuroSys), April 2015. [doi:10.1145/2741948.2741968](http://dx.doi.org/10.1145/2741948.2741968)

1.  Aapo Kyrola, Guy Blelloch, and Carlos Guestrin: “[GraphChi: Large-Scale Graph Computation on Just a PC](https://www.usenix.org/system/files/conference/osdi12/osdi12-final-126.pdf),” at *10th USENIX Symposium on Operating Systems Design and Implementation* (OSDI), October 2012.

1.  Andrew Lenharth, Donald Nguyen, and Keshav Pingali: “[Parallel Graph Analytics](http://cacm.acm.org/magazines/2016/5/201591-parallel-graph-analytics/fulltext),” *Communications of the ACM*, volume 59, number 5, pages 78–87, May [doi:10.1145/2901919](http://dx.doi.org/10.1145/2901919)

1.  Fabian Hüske: “[Peeking into Apache Flink's Engine Room](http://flink.apache.org/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html),” *flink.apache.org*, March 13, 2015.

1.  Mostafa Mokhtar: “[Hive 0.14 Cost Based Optimizer (CBO) Technical Overview](http://hortonworks.com/blog/hive-0-14-cost-based-optimizer-cbo-technical-overview/),” *hortonworks.com*, March 2, 2015.

1.  Michael Armbrust, Reynold S Xin, Cheng Lian, et al.: “[Spark SQL: Relational Data Processing in Spark](http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf),” at *ACM International Conference on Management of Data* (SIGMOD), June 2015. [doi:10.1145/2723372.2742797](http://dx.doi.org/10.1145/2723372.2742797)

1.  Daniel Blazevski: “[Planting Quadtrees for Apache Flink](http://insightdataengineering.com/blog/flink-knn/),” *insightdataengineering.com*, March 25, 2016.

1.  Tom White: “[Genome Analysis Toolkit: Now Using Apache Spark for Data Processing](http://blog.cloudera.com/blog/2016/04/genome-analysis-toolkit-now-using-apache-spark-for-data-processing/),” *blog.cloudera.com*, April 6, 2016.

------

| 上一章                            | 目錄                            | 下一章                   |
| --------------------------------- | ------------------------------- | ------------------------ |
| [第三部分：派生資料](part-iii.md) | [設計資料密集型應用](README.md) | [第十一章：流處理](ch11.md) |

