# 6. 分區 

![](img/ch6.png)

> 我們必須跳出電腦指令序列的窠臼。 敘述定義、描述中繼資料、梳理關係，而不是編寫過程。
>
> —— Grace Murray Hopper，未來的電腦及其管理（1962）
>

-------------

[TOC]

   在[第5章](ch5.md)中，我們討論了複製——即資料在不同節點上的副本,對於非常大的資料集,或非常高的輸送量，僅僅進行複製是不夠的：我們需要將資料進行**分區（partitions）**，也稱為**分片（sharding）**[^i]

[^i]: 正如本章所討論的，分區是一種有意將大型資料庫分解成小型資料庫的方式。它與**網路磁碟分割（net splits）**無關，這是節點之間網路中的一種故障類型。我們將在[第8章](ch8.md)討論這些錯誤。

> ##### 術語澄清
>
>  上文中的**分區(partition)**,在MongoDB,Elasticsearch和Solr Cloud中被稱為**分片(shard)**,在HBase中稱之為**區域(Region)**，Bigtable中則是 **表塊（tablet）**，Cassandra和Riak中是**虛節點（vnode)**, Couchbase中叫做**虛桶(vBucket)**.但是**分區(partition)** 是約定俗成的叫法。
>

   通常情況下，每條資料（每條記錄，每行或每個文檔）屬於且僅屬於一個分區。有很多方法可以實現這一點，本章將進行深入討論。實際上，每個分區都是自己的小型資料庫，儘管資料庫可能支援同時進行多個分區的操作。

   分區主要是為了**可擴展性**。不同的分區可以放在不共用集群中的不同節點上（參閱[第二部分](part-ii.md)關於[無共用架構](part-ii.md#無共用架構)的定義）。因此，大資料集可以分佈在多個磁片上，並且查詢負載可以分佈在多個處理器上。

   對於在單個分區上運行的查詢，每個節點可以獨立執行對自己的查詢，因此可以通過添加更多的節點來擴大查詢輸送量。大型，複雜的查詢可能會跨越多個節點並行處理，儘管這也帶來了新的困難。

   分區資料庫在20世紀80年代由Teradata和NonStop SQL【1】等產品率先推出，最近因為NoSQL資料庫和基於Hadoop的資料倉庫重新被關注。有些系統是為事務性工作設計的，有些系統則用於分析（參閱“[交易處理或分析]”）：這種差異會影響系統的運作方式，但是分區的基本原理均適用於這兩種工作方式。

   在本章中，我們將首先介紹分割大型資料集的不同方法，並觀察索引如何與分區配合。然後我們將討論[重新平衡分區](#重新平衡分區)，如果想要添加或刪除群集中的節點，則必須進行再平衡。最後，我們將概述資料庫如何將請求路由到正確的分區並執行查詢。

## 分區與複製

   分區通常與複製結合使用，使得每個分區的副本存儲在多個節點上。 這意味著，即使每條記錄屬於一個分區，它仍然可以存儲在多個不同的節點上以獲得容錯能力。

   一個節點可能存儲多個分區。 如果使用主從複製模型，則分區和複製的組合如[圖6-1]()所示。 每個分區領導者(主)被分配給一個節點，追隨者(從)被分配給其他節點。 每個節點可能是某些分區的領導者，同時是其他分區的追隨者。
我們在[第5章](ch5.md)討論的關於資料庫複製的所有內容同樣適用於分區的複製。 大多數情況下，分區方案的選擇與複製方案的選擇是獨立的，為簡單起見，本章中將忽略複製。

![](img/fig6-1.png)

**圖6-1 組合使用複製和分區：每個節點充當某些分區的領導者，其他分區充當追隨者。**

## 鍵值數據的分區

   假設你有大量資料並且想要分區,如何決定在哪些節點上存儲哪些記錄呢？

   分區目標是將資料和查詢負載均勻分佈在各個節點上。如果每個節點公平分享資料和負載，那麼理論上10個節點應該能夠處理10倍的資料量和10倍的單個節點的讀寫輸送量（暫時忽略複製）。

   如果分區是不公平的，一些分區比其他分區有更多的資料或查詢，我們稱之為**偏斜（skew）**。資料偏斜的存在使分區效率下降很多。在極端的情況下，所有的負載可能壓在一個分區上，其餘9個節點空閒的，瓶頸落在這一個繁忙的節點上。不均衡導致的高負載的分區被稱為**熱點（hot spot）**。

   避免熱點最簡單的方法是將記錄隨機分配給節點。這將在所有節點上平均分配資料，但是它有一個很大的缺點：當你試圖讀取一個特定的值時，你無法知道它在哪個節點上，所以你必須並行地查詢所有的節點。

   我們可以做得更好。現在假設您有一個簡單的鍵值資料模型，其中您總是通過其主鍵訪問記錄。例如，在一本老式的紙質百科全書中，你可以通過標題來查找一個條目;由於所有條目按字母順序排序，因此您可以快速找到您要查找的條目。

### 根據鍵的範圍分區

   一種分區的方法是為每個分區指定一塊連續的鍵範圍（從最小值到最大值），如紙百科全書的卷（[圖6-2]()）。如果知道範圍之間的邊界，則可以輕鬆確定哪個分區包含某個值。如果您還知道分區所在的節點，那麼可以直接向相應的節點發出請求（對於百科全書而言，就像從書架上選取正確的書籍）。

![](img/fig6-2.png)

**圖6-2 印刷版百科全書按照關鍵字範圍進行分區**

   鍵的範圍不一定均勻分佈，因為資料也很可能不均勻分佈。例如在[圖6-2]()中，第1卷包含以A和B開頭的單詞，但第12卷則包含以T，U，V，X，Y和Z開頭的單詞。只是簡單的規定每個卷包含兩個字母會導致一些卷比其他卷大。為了均勻分配資料，分區邊界需要依據資料調整。

   分區邊界可以由管理員手動選擇，也可以由資料庫自動選擇（我們會在“[重新平衡分區]()”中更詳細地討論分區邊界的選擇）。 Bigtable使用了這種分區策略，以及其開源等價物HBase 【2, 3】，RethinkDB和2.4版本之前的MongoDB 【4】。

   在每個分區中，我們可以按照一定的順序保存鍵（參見“[SSTables和LSM-樹]()”）。好處是進行範圍掃描非常簡單，您可以將鍵作為聯合索引來處理，以便在一次查詢中獲取多個相關記錄（參閱“[多列索引](#ch2.md#多列索引)”）。例如，假設我們有一個程式來存儲感測器網路的資料，其中主鍵是測量的時間戳記（年月日時分秒）。範圍掃描在這種情況下非常有用，因為我們可以輕鬆獲取某個月份的所有資料。

   然而，Key Range分區的缺點是某些特定的訪問模式會導致熱點。 如果主鍵是時間戳記，則分區對應於時間範圍，例如，給每天分配一個分區。 不幸的是，由於我們在測量發生時將資料從感測器寫入資料庫，因此所有寫入操作都會轉到同一個分區（即今天的分區），這樣分區可能會因寫入而超載，而其他分區則處於空閒狀態【5】。

   為了避免感測器資料庫中的這個問題，需要使用除了時間戳記以外的其他東西作為主鍵的第一個部分。 例如，可以在每個時間戳記前添加感測器名稱，這樣會首先按感測器名稱，然後按時間進行分區。 假設有多個感測器同時運行，寫入負載將最終均勻分佈在不同分區上。 現在，當想要在一個時間範圍內獲取多個感測器的值時，您需要為每個感測器名稱執行一個單獨的範圍查詢。

### 根據鍵的散列分區

   由於偏斜和熱點的風險，許多分散式資料存儲使用散列函數來確定給定鍵的分區。

   一個好的散列函數可以將將偏斜的資料均勻分佈。假設你有一個32位元散列函數,無論何時給定一個新的字串輸入，它將返回一個0到$2^{32}$ -1之間的"隨機"數。即使輸入的字串非常相似，它們的散列也會均勻分佈在這個數字範圍內。

   出於分區的目的，散列函數不需要多麼強壯的加密演算法：例如，Cassandra和MongoDB使用MD5，Voldemort使用Fowler-Noll-Vo函數。許多程式設計語言都有內置的簡單雜湊函數（它們用於雜湊表），但是它們可能不適合分區：例如，在Java的`Object.hashCode()`和Ruby的`Object#hash`，同一個鍵可能在不同的進程中有不同的雜湊值【6】。

   一旦你有一個合適的鍵散列函數，你可以為每個分區分配一個散列範圍（而不是鍵的範圍），每個通過雜湊散列落在分區範圍內的鍵將被存儲在該分區中。如[圖6-3](img/fig6-3.png)所示。

![](img/fig6-3.png)

**圖6-3 按雜湊鍵分區**

   這種技術擅長在分區之間分配鍵。分區邊界可以是均勻間隔的，也可以是偽隨機選擇的（在這種情況下，該技術有時也被稱為**一致性雜湊（consistent hashing）**）。

> #### 一致性雜湊
>
>  一致性雜湊由Karger等人定義。【7】 用於跨互聯網級別的緩存系統，例如CDN中，是一種能均勻分配負載的方法。它使用隨機選擇的**分區邊界（partition boundaries）**來避免中央控制或分散式一致性的需要。 請注意，這裡的一致性與複製一致性（請參閱第5章）或ACID一致性（參閱[第7章](ch7.md)）無關，而是描述了重新平衡的特定方法。
>
>  正如我們將在“[重新平衡分區](#重新平衡分區)”中所看到的，這種特殊的方法對於資料庫實際上並不是很好，所以在實際中很少使用（某些資料庫的文檔仍然指的是一致性雜湊，但是它 往往是不準確的）。 因為有可能產生混淆，所以最好避免使用一致性雜湊這個術語，而只是把它稱為**散列分區（hash partitioning）**。

   不幸的是，通過使用Key散列進行分區，我們失去了鍵範圍分區的一個很好的屬性：高效執行範圍查詢的能力。曾經相鄰的金鑰現在分散在所有分區中，所以它們之間的順序就丟失了。在MongoDB中，如果您使用了基於散列的分區模式，則任何範圍查詢都必須發送到所有分區【4】。Riak 【9】，Couchbase 【10】或Voldemort不支援主鍵上的範圍查詢。

   Cassandra採取了折衷的策略【11, 12, 13】。 Cassandra中的表可以使用由多個列組成的複合主鍵來聲明。鍵中只有第一列會作為散列的依據，而其他列則被用作Casssandra的SSTables中排序資料的連接索引。儘管查詢無法在複合主鍵的第一列中按範圍掃表，但如果第一列已經指定了固定值，則可以對該鍵的其他列執行有效的範圍掃描。

   組合索引方法為一對多關聯性提供了一個優雅的資料模型。例如，在社交媒體網站上，一個用戶可能會發佈很多更新。如果更新的主鍵被選擇為`(user_id, update_timestamp)`，那麼您可以有效地檢索特定用戶在某個時間間隔內按時間戳記排序的所有更新。不同的使用者可以存儲在不同的分區上，對於每個使用者，更新按時間戳記順序存儲在單個分區上。

### 負載傾斜與消除熱點

   如前所述，雜湊分區可以幫助減少熱點。但是，它不能完全避免它們：在極端情況下，所有的讀寫操作都是針對同一個鍵的，所有的請求都會被路由到同一個分區。

   這種場景也許並不常見，但並非聞所未聞：例如，在社交媒體網站上，一個擁有數百萬追隨者的名人用戶在做某事時可能會引發一場風暴【14】。這個事件可能導致大量寫入同一個鍵（鍵可能是名人的用戶ID，或者人們正在評論的動作的ID）。雜湊策略不起作用，因為兩個相同ID的雜湊值仍然是相同的。

   如今，大多數資料系統無法自動補償這種高度偏斜的負載，因此應用程式有責任減少偏斜。例如，如果一個主鍵被認為是非常火爆的，一個簡單的方法是在主鍵的開始或結尾添加一個亂數。只要一個兩位數的十進位亂數就可以將主鍵分散為100鐘不同的主鍵,從而存儲在不同的分區中。

   然而，將主鍵進行分割之後，任何讀取都必須要做額外的工作，因為他們必須從所有100個主鍵分佈中讀取資料並將其合併。此技術還需要額外的記錄：只需要對少量熱點附加亂數;對於寫入輸送量低的絕大多數主鍵來是不必要的開銷。因此，您還需要一些方法來跟蹤哪些鍵需要被分割。

   也許在將來，資料系統將能夠自動檢測和補償偏斜的工作負載；但現在，您需要自己來權衡。

## 分片與次級索引

   到目前為止，我們討論的分區方案依賴於鍵值資料模型。如果只通過主鍵訪問記錄，我們可以從該鍵確定分區，並使用它來將讀寫請求路由到負責該鍵的分區。

   如果涉及次級索引，情況會變得更加複雜（參考“[其他索引結構]()”）。輔助索引通常並不能唯一地標識記錄，而是一種搜索記錄中出現特定值的方式：查找用戶123的所有操作，查找包含詞語`hogwash`的所有文章，查找所有顏色為紅色的車輛等等。

   次級索引是關係型數據庫的基礎，並且在文檔資料庫中也很普遍。許多鍵值存儲（如HBase和Volde-mort）為了減少實現的複雜度而放棄了次級索引，但是一些（如Riak）已經開始添加它們，因為它們對於資料模型實在是太有用了。並且次級索引也是Solr和Elasticsearch等搜索伺服器的基石。

   次級索引的問題是它們不能整齊地映射到分區。有兩種用二級索引對資料庫進行分區的方法：**基於文檔的分區（document-based）**和**基於關鍵字（term-based）的分區**。

### 按文檔的二級索引

   假設你正在經營一個銷售二手車的網站（如[圖6-4](img/fig6-4.png)所示）。 每個列表都有一個唯一的ID——稱之為文檔ID——並且用文檔ID對資料庫進行分區（例如，分區0中的ID 0到499，分區1中的ID 500到999等）。

   你想讓用戶搜索汽車，允許他們通過顏色和廠商過濾，所以需要一個在顏色和廠商上的次級索引（文檔資料庫中這些是**欄位（field）**，關聯式資料庫中這些是**列（column）** ）。 如果您聲明瞭索引，則資料庫可以自動執行索引[^ii]。例如，無論何時將紅色汽車添加到資料庫，資料庫分區都會自動將其添加到索引條目`color：red`的文檔ID列表中。

[^ii]: 如果資料庫僅支援鍵值模型，則你可能會嘗試在應用程式碼中創建從值到文檔ID的映射來實現輔助索引。 如果沿著這條路線走下去，請萬分小心，確保您的索引與底層資料保持一致。 競爭條件和間歇性寫入失敗（其中一些更改已保存，但其他更改未保存）很容易導致資料不同步 - 參見“[多物件事務的需求]()”。

![](img/fig6-4.png)

**圖6-4 按文檔分區二級索引**

   在這種索引方法中，每個分區是完全獨立的：每個分區維護自己的二級索引，僅覆蓋該分區中的文檔。它不關心存儲在其他分區的資料。無論何時您需要寫入資料庫（添加，刪除或更新文檔），只需處理包含您正在編寫的文檔ID的分區即可。出於這個原因，**文檔分區索引**也被稱為**本地索引（local index）**（而不是將在下一節中描述的**全域索引（global index）**）。

   但是，從文檔分區索引中讀取需要注意：除非您對文檔ID做了特別的處理，否則沒有理由將所有具有特定顏色或特定品牌的汽車放在同一個分區中。在[圖6-4](img/fig6-4.png)中，紅色汽車出現在分區0和分區1中。因此，如果要搜索紅色汽車，則需要將查詢發送到所有分區，併合並所有返回的結果。

   這種查詢分區資料庫的方法有時被稱為**分散/聚集（scatter/gather）**，並且可能會使二級索引上的讀取查詢相當昂貴。即使並行查詢分區，分散/聚集也容易導致尾部延遲放大（參閱“[實踐中的百分位點](ch1.md#實踐中的百分位點)”）。然而，它被廣泛使用：MongoDB，Riak 【15】，Cassandra 【16】，Elasticsearch 【17】，SolrCloud 【18】和VoltDB 【19】都使用文檔分區二級索引。大多數資料庫供應商建議您構建一個能從單個分區提供二級索引查詢的分區方案，但這並不總是可行，尤其是當在單個查詢中使用多個二級索引時（例如同時需要按顏色和製造商查詢）。

### 根據關鍵字(Term)的二級索引

   我們可以構建一個覆蓋所有分區資料的**全域索引**，而不是給每個分區創建自己的次級索引（本地索引）。但是，我們不能只把這個索引存儲在一個節點上，因為它可能會成為瓶頸，違背了分區的目的。全域索引也必須進行分區，但可以採用與主鍵不同的分區方式。

   [圖6-5](img/fig6-5.png)述了這可能是什麼樣子：來自所有分區的紅色汽車在紅色索引中，並且索引是分區的，首字母從`a`到`r`的顏色在分區0中，`s`到`z`的在分區1。汽車製造商的索引也與之類似（分區邊界在`f`和`h`之間）。

![](img/fig6-5.png)

**圖6-5 按關鍵字對二級索引進行分區**

   我們將這種索引稱為**關鍵字分區（term-partitioned）**，因為我們尋找的關鍵字決定了索引的分區方式。例如，一個關鍵字可能是：`顏色：紅色`。**關鍵字(Term)** 來源於來自全文檢索搜尋索引（一種特殊的次級索引），指文檔中出現的所有單詞。

   和之前一樣，我們可以通過**關鍵字**本身或者它的散列進行索引分區。根據它本身分區對於範圍掃描非常有用（例如對於數字,像汽車的報價），而對關鍵字的雜湊分區提供了負載均衡的能力。

   關鍵字分區的全域索引優於文檔分區索引的地方點是它可以使讀取更有效率：不需要**分散/收集**所有分區，用戶端只需要向包含關鍵字的分區發出請求。全域索引的缺點在於寫入速度較慢且較為複雜，因為寫入單個文檔現在可能會影響索引的多個分區（文檔中的每個關鍵字可能位於不同的分區或者不同的節點上） 。

   理想情況下，索引總是最新的，寫入資料庫的每個文檔都會立即反映在索引中。但是，在關鍵字分區索引中，這需要跨分區的分散式事務，並不是所有資料庫都支援（請參閱[第7章](ch7.md)和[第9章](ch9.md)）。

   在實踐中，對全域二級索引的更新通常是**非同步**的（也就是說，如果在寫入之後不久讀取索引，剛才所做的更改可能尚未反映在索引中）。例如，Amazon DynamoDB聲稱在正常情況下，其全域次級索引會在不到一秒的時間內更新，但在基礎架構出現故障的情況下可能會有延遲【20】。

   全域關鍵字分區索引的其他用途包括Riak的搜索功能【21】和Oracle資料倉庫，它允許您在本地和全域索引之間進行選擇【22】。我們將在[第12章](ch12.md)中涉及實現關鍵字二級索引的話題。

## 分區再平衡

隨著時間的推移，資料庫會有各種變化。

* 查詢輸送量增加，所以您想要添加更多的CPU來處理負載。
* 資料集大小增加，所以您想添加更多的磁片和RAM來存儲它。
* 機器出現故障，其他機器需要接管故障機器的責任。

所有這些更改都需要資料和請求從一個節點移動到另一個節點。 將負載從集群中的一個節點向另一個節點移動的過程稱為**再平衡（reblancing）**。

無論使用哪種分區方案，再平衡通常都要滿足一些最低要求：

* 再平衡之後，負載（資料存儲，讀取和寫入請求）應該在集群中的節點之間公平地共用。
* 再平衡發生時，資料庫應該繼續接受讀取和寫入。
* 節點之間只移動必須的資料，以便快速再平衡，並減少網路和磁片I/O負載。

### 平衡策略

有幾種不同的分區分配方法【23】,讓我們依次簡要討論一下。

#### 反面教材：hash mod N

   我們在前面說過（[圖6-3](img/fig6-3.png)），最好將可能的散列分成不同的範圍，並將每個範圍分配給一個分區（例如，如果$0≤hash(key)<b_0$，則將鍵分配給分區0，如果$b_0 ≤ hash(key) <b_1$，則分配給分區1）

   也許你想知道為什麼我們不使用***mod***（許多程式設計語言中的％運運算元）。例如，`hash(key) mod 10`會返回一個介於0和9之間的數字（如果我們將散列寫為十進位數字，散列模10將是最後一個數字）。如果我們有10個節點，編號為0到9，這似乎是將每個鍵分配給一個節點的簡單方法。

   模$N$方法的問題是，如果節點數量N發生變化，大多數金鑰將需要從一個節點移動到另一個節點。例如，假設$hash(key)=123456$。如果最初有10個節點，那麼這個鍵一開始放在節點6上（因為$123456\ mod\  10 = 6$）。當您增長到11個節點時，金鑰需要移動到節點3（$123456\ mod\ 11 = 3$），當您增長到12個節點時，需要移動到節點0（$123456\ mod\ 12 = 0$）。這種頻繁的舉動使得重新平衡過於昂貴。

   我們需要一種只移動必需資料的方法。

#### 固定數量的分區

   幸運的是，有一個相當簡單的解決方案：創建比節點更多的分區，並為每個節點分配多個分區。例如，運行在10個節點的集群上的資料庫可能會從一開始就被拆分為1,000個分區，因此大約有100個分區被分配給每個節點。

   現在，如果一個節點被添加到集群中，新節點可以從當前每個節點中**竊取**一些分區，直到分區再次公平分配。這個過程如[圖6-6](img/fig6-6.png)所示。如果從集群中刪除一個節點，則會發生相反的情況。

   只有分區在節點之間的移動。分區的數量不會改變，鍵所指定的分區也不會改變。唯一改變的是分區所在的節點。這種變更並不是即時的 — 在網路上傳輸大量的資料需要一些時間 — 所以在傳輸過程中，原有分區仍然會接受讀寫操作。

![](img/fig6-6.png)

**圖6-6 將新節點添加到每個節點具有多個分區的資料庫群集。**

   原則上，您甚至可以解決集群中的硬體不匹配問題：通過為更強大的節點分配更多的分區，可以強制這些節點承載更多的負載。在Riak 【15】，Elasticsearch 【24】，Couchbase 【10】和Voldemort 【25】中使用了這種再平衡的方法。

   在這種配置中，分區的數量通常在資料庫第一次建立時確定，之後不會改變。雖然原則上可以分割和合併分區（請參閱下一節），但固定數量的分區在操作上更簡單，因此許多固定分區資料庫選擇不實施分區分割。因此，一開始配置的分區數就是您可以擁有的最大節點數量，所以您需要選擇足夠多的分區以適應未來的增長。但是，每個分區也有管理開銷，所以選擇太大的數字會適得其反。

   如果資料集的總大小難以預估（例如，如果它開始很小，但隨著時間的推移可能會變得更大），選擇正確的分區數是困難的。由於每個分區包含了總數據量固定比率的資料，因此每個分區的大小與集群中的資料總量成比例增長。如果分區非常大，再平衡和從節點故障恢復變得昂貴。但是，如果分區太小，則會產生太多的開銷。當分區大小“恰到好處”的時候才能獲得很好的性能，如果分區數量固定，但資料量變動很大，則難以達到最佳性能。

#### 動態分區

   對於使用鍵範圍分區的資料庫（參閱“[按鍵範圍分區](#按鍵範圍分區)”），具有固定邊界的固定數量的分區將非常不便：如果出現邊界錯誤，則可能會導致一個分區中的所有資料或者其他分區中的所有資料為空。手動重新配置分區邊界將非常繁瑣。

   出於這個原因，按鍵的範圍進行分區的資料庫（如HBase和RethinkDB）會動態創建分區。當分區增長到超過配置的大小時（在HBase上，預設值是10GB），會被分成兩個分區，每個分區約占一半的資料【26】。與之相反，如果大量資料被刪除並且分區縮小到某個閾值以下，則可以將其與相鄰分區合併。此過程與B樹頂層發生的過程類似（參閱“[B樹](ch2.md#B樹)”）。

   每個分區分配給一個節點，每個節點可以處理多個分區，就像固定數量的分區一樣。大型分區拆分後，可以將其中的一半轉移到另一個節點，以平衡負載。在HBase中，分區檔的傳輸通過HDFS（底層分散式檔案系統）來實現【3】。

   動態分區的一個優點是分區數量適應總數據量。如果只有少量的資料，少量的分區就足夠了，所以開銷很小;如果有大量的資料，每個分區的大小被限制在一個可配置的最大值【23】。

   需要注意的是，一個空的資料庫從一個分區開始，因為沒有關於在哪裡繪製分區邊界的先驗資訊。資料集開始時很小，直到達到第一個分區的分割點，所有寫入操作都必須由單個節點處理，而其他節點則處於空閒狀態。為瞭解決這個問題，HBase和MongoDB允許在一個空的資料庫上配置一組初始分區（這被稱為**預分割（pre-splitting）**）。在鍵範圍分區的情況中，預分割需要提前知道鍵是如何進行分配的【4,26】。

   動態分區不僅適用於資料的範圍分區，而且也適用於散列分區。從版本2.4開始，MongoDB同時支持範圍和雜湊分區，並且都是進行動態分割分區。

#### 按節點比例分區

   通過動態分區，分區的數量與資料集的大小成正比，因為拆分和合併過程將每個分區的大小保持在固定的最小值和最大值之間。另一方面，對於固定數量的分區，每個分區的大小與資料集的大小成正比。在這兩種情況下，分區的數量都與節點的數量無關。

   Cassandra和Ketama使用的第三種方法是使分區數與節點數成正比——換句話說，每個節點具有固定數量的分區【23,27,28】。在這種情況下，每個分區的大小與資料集大小成比例地增長，而節點數量保持不變，但是當增加節點數時，分區將再次變小。由於較大的資料量通常需要較大數量的節點進行存儲，因此這種方法也使每個分區的大小較為穩定。

   當一個新節點加入集群時，它隨機選擇固定數量的現有分區進行拆分，然後佔有這些拆分分區中每個分區的一半，同時將每個分區的另一半留在原地。隨機化可能會產生不公平的分割，但是平均在更大數量的分區上時（在Cassandra中，預設情況下，每個節點有256個分區），新節點最終從現有節點獲得公平的負載份額。 Cassandra 3.0引入了另一種再分配的演算法來避免不公平的分割【29】。

   隨機選擇分區邊界要求使用基於散列的分區（可以從散列函數產生的數位範圍中挑選邊界）。實際上，這種方法最符合一致性雜湊的原始定義【7】（參閱“[一致性雜湊](#一致性雜湊)”）。最新的雜湊函數可以在較低中繼資料開銷的情況下達到類似的效果【8】。

### 運維：手動還是自動平衡

   關於再平衡有一個重要問題：自動還是手動進行？

   在全自動重新平衡（系統自動決定何時將分區從一個節點移動到另一個節點，無須人工幹預）和完全手動（分區指派給節點由管理員明確配置，僅在管理員明確重新配置時才會更改）之間有一個權衡。例如，Couchbase，Riak和Voldemort會自動生成建議的分區分配，但需要管理員提交才能生效。

   全自動重新平衡可以很方便，因為正常維護的操作工作較少。但是，這可能是不可預測的。再平衡是一個昂貴的操作，因為它需要重新路由請求並將大量資料從一個節點移動到另一個節點。如果沒有做好，這個過程可能會使網路或節點負載過重，降低其他請求的性能。

   這種自動化與自動故障檢測相結合可能十分危險。例如，假設一個節點超載，並且對請求的回應暫時很慢。其他節點得出結論：超載的節點已經死亡，並自動重新平衡集群，使負載離開它。這會對已經超負荷的節點，其他節點和網路造成額外的負載，從而使情況變得更糟，並可能導致級聯失敗。

   出於這個原因，再平衡的過程中有人參與是一件好事。這比完全自動的過程慢，但可以幫助防止運維意外。

## 請求路由

   現在我們已經將資料集分割到多個機器上運行的多個節點上。但是仍然存在一個懸而未決的問題：當客戶想要發出請求時，如何知道要連接哪個節點？隨著分區重新平衡，分區對節點的分配也發生變化。為了回答這個問題，需要有人知曉這些變化：如果我想讀或寫鍵“foo”，需要連接哪個IP位址和埠號？

   這個問題可以概括為 **服務發現(service discovery)** ，它不僅限於資料庫。任何可通過網路訪問的軟體都有這個問題，特別是如果它的目標是高可用性（在多台機器上運行冗餘配置）。許多公司已經編寫了自己的內部服務發現工具，其中許多已經作為開源發佈【30】。

概括來說，這個問題有幾種不同的方案（如圖6-7所示）:

1. 允許客戶聯繫任何節點（例如，通過**迴圈策略的負載均衡（Round-Robin Load Balancer）**）。如果該節點恰巧擁有請求的分區，則它可以直接處理該請求;否則，它將請求轉發到適當的節點，接收回復並傳遞給用戶端。
2. 首先將所有來自用戶端的請求發送到路由層，它決定了應該處理請求的節點，並相應地轉發。此路由層本身不處理任何請求；它僅負責分區的負載均衡。
3. 要求用戶端知道分區和節點的分配。在這種情況下，用戶端可以直接連接到適當的節點，而不需要任何仲介。

以上所有情況中的關鍵問題是：作出路由決策的元件（可能是節點之一，還是路由層或用戶端）如何瞭解分區-節點之間的分配關係變化？

![](img/fig6-7.png)

**圖6-7 將請求路由到正確節點的三種不同方式。**

   這是一個具有挑戰性的問題，因為重要的是所有參與者都同意 - 否則請求將被發送到錯誤的節點，而不是正確處理。 在分散式系統中有達成共識的協定，但很難正確地實現（見[第9章](ch9.md)）。

   許多分散式資料系統都依賴於一個獨立的協調服務，比如ZooKeeper來跟蹤集群中繼資料，如[圖6-8](img/fig6-8.png)所示。 每個節點在ZooKeeper中註冊自己，ZooKeeper維護分區到節點的可靠映射。 其他參與者（如路由層或分區感知用戶端）可以在ZooKeeper中訂閱此資訊。 只要分區分配發生的改變，或者集群中添加或刪除了一個節點，ZooKeeper就會通知路由層使路由資訊保持最新狀態。

![](img/fig6-8.png)

**圖6-8 使用ZooKeeper跟蹤分區分配給節點。**

   例如，LinkedIn的Espresso使用Helix 【31】進行集群管理（依靠ZooKeeper），實現了如[圖6-8](img/fig6-8.png)所示的路由層。 HBase，SolrCloud和Kafka也使用ZooKeeper來跟蹤分區分配。 MongoDB具有類似的體系結構，但它依賴於自己的**配置伺服器（config server）** 實現和mongos守護進程作為路由層。

   Cassandra和Riak採取不同的方法：他們在節點之間使用**流言協議（gossip protocol）** 來傳播群集狀態的變化。請求可以發送到任意節點，該節點會轉發到包含所請求的分區的適當節點（[圖6-7]()中的方法1）。這個模型在資料庫節點中增加了更多的複雜性，但是避免了對像ZooKeeper這樣的外部協調服務的依賴。

   Couchbase不會自動重新平衡，這簡化了設計。通常情況下，它配置了一個名為moxi的路由層，它會從集群節點瞭解路由變化【32】。

   當使用路由層或向隨機節點發送請求時，用戶端仍然需要找到要連接的IP位址。這些位址並不像分區的節點分佈變化的那麼快，所以使用DNS通常就足夠了。

### 執行並行查詢

   到目前為止，我們只關注讀取或寫入單個鍵的非常簡單的查詢（對於文檔分區的二級索引，另外還有分散/聚集查詢）。這與大多數NoSQL分散式資料存儲所支援的訪問級別有關。

   然而，通常用於分析的**大規模並行處理（MPP, Massively parallel processing）** 關係型數據庫產品在其支援的查詢類型方面要複雜得多。一個典型的資料倉庫查詢包含多個連接，過濾，分組和聚合操作。 MPP查詢最佳化工具將這個複雜的查詢分解成許多執行階段和分區，其中許多可以在資料庫集群的不同節點上並存執行。涉及掃描大規模資料集的查詢特別受益於這種並存執行。

   資料倉庫查詢的快速並存執行是一個專門的話題，由於分析有很重要的商業意義，可以帶來很多利益。我們將在第10章討論並行查詢執行的一些技巧。有關並行資料庫中使用的技術的更詳細的概述，請參閱參考文獻【1,33】。

## 本章小結

   在本章中，我們探討了將大資料集劃分成更小的子集的不同方法。資料量非常大的時候，在單台機器上存儲和處理不再可行，則分區十分必要。分區的目標是在多台機器上均勻分佈資料和查詢負載，避免出現熱點（負載不成比例的節點）。這需要選擇適合於您的資料的分區方案，並在將節點添加到集群或從集群刪除時進行再分區。

我們討論了兩種主要的分區方法：

***鍵範圍分區***

   其中鍵是有序的，並且分區擁有從某個最小值到某個最大值的所有鍵。排序的優勢在於可以進行有效的範圍查詢，但是如果應用程式經常訪問相鄰的主鍵，則存在熱點的風險。

   在這種方法中，當分區變得太大時，通常將分區分成兩個子分區，動態地再平衡分區。

***散列分區***

   散列函數應用於每個鍵，分區擁有一定範圍的散列。這種方法破壞了鍵的排序，使得範圍查詢效率低下，但可以更均勻地分配負載。

   通過散列進行分區時，通常先提前創建固定數量的分區，為每個節點分配多個分區，並在添加或刪除節點時將整個分區從一個節點移動到另一個節點。也可以使用動態分區。

兩種方法搭配使用也是可行的，例如使用複合主鍵：使用鍵的一部分來標識分區，而使用另一部分作為排序順序。

我們還討論了分區和二級索引之間的相互作用。次級索引也需要分區，有兩種方法：

* 按文檔分區（本地索引），其中二級索引存儲在與主鍵和值相同的分區中。這意味著只有一個分區需要在寫入時更新，但是讀取二級索引需要在所有分區之間進行分散/收集。
* 按關鍵字分區（全域索引），其中二級索引存在不同的分區中。輔助索引中的條目可以包括來自主鍵的所有分區的記錄。當文檔寫入時，需要更新多個分區中的二級索引；但是可以從單個分區中進行讀取。

最後，我們討論了將查詢路由到適當的分區的技術，從簡單的分區負載平衡到複雜的並行查詢執行引擎。

   按照設計，多數情況下每個分區是獨立運行的 — 這就是分區資料庫可以擴展到多台機器的原因。但是，需要寫入多個分區的操作結果可能難以預料：例如，如果寫入一個分區成功，但另一個分區失敗，會發生什麼情況？我們將在下麵的章節中討論這個問題。

參考文獻
--------------------

1.  David J. DeWitt and Jim N. Gray:    “[Parallel Database Systems: The Future of High Performance Database Systems](),”
    *Communications of the ACM*, volume 35, number 6, pages 85–98, June 1992. [doi:10.1145/129888.129894](http://dx.doi.org/10.1145/129888.129894)

2.  Lars George:    “[HBase vs. BigTable Comparison](http://www.larsgeorge.com/2009/11/hbase-vs-bigtable-comparison.html),” *larsgeorge.com*, November 2009.

3.  “[The Apache HBase Reference Guide](https://hbase.apache.org/book/book.html),” Apache Software Foundation, *hbase.apache.org*, 2014.

4.  MongoDB, Inc.:    “[New Hash-Based Sharding Feature in MongoDB 2.4](http://blog.mongodb.org/post/47633823714/new-hash-based-sharding-feature-in-mongodb-24),” *blog.mongodb.org*, April 10, 2013.

5.  Ikai Lan:    “[App Engine Datastore Tip: Monotonically Increasing Values Are Bad](http://ikaisays.com/2011/01/25/app-engine-datastore-tip-monotonically-increasing-values-are-bad/),” *ikaisays.com*,
    January 25, 2011.

6.  Martin Kleppmann:    “[Java's hashCode Is Not Safe for Distributed Systems](http://martin.kleppmann.com/2012/06/18/java-hashcode-unsafe-for-distributed-systems.html),” *martin.kleppmann.com*, June 18, 2012.

7.  David Karger, Eric Lehman, Tom Leighton, et al.:    “[Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web](http://www.akamai.com/dl/technical_publications/ConsistenHashingandRandomTreesDistributedCachingprotocolsforrelievingHotSpotsontheworldwideweb.pdf),” at *29th Annual ACM Symposium on Theory of Computing* (STOC), pages 654–663, 1997. [doi:10.1145/258533.258660](http://dx.doi.org/10.1145/258533.258660)

8.  John Lamping and Eric Veach:    “[A Fast, Minimal Memory, Consistent Hash Algorithm](http://arxiv.org/pdf/1406.2294v1.pdf),” *arxiv.org*, June 2014.

9.  Eric Redmond:    “[A Little Riak Book](http://littleriakbook.com/),” Version 1.4.0, Basho Technologies, September 2013.

10.  “[Couchbase 2.5 Administrator Guide](http://docs.couchbase.com/couchbase-manual-2.5/cb-admin/),” Couchbase, Inc., 2014.

11.  Avinash Lakshman and Prashant Malik:     “[Cassandra – A Decentralized Structured Storage System](http://www.cs.cornell.edu/Projects/ladis2009/papers/Lakshman-ladis2009.PDF),” at *3rd ACM SIGOPS International Workshop on
     Large Scale Distributed Systems and Middleware* (LADIS), October 2009.

12.  Jonathan Ellis:     “[Facebook’s Cassandra Paper, Annotated and Compared to Apache Cassandra 2.0](http://www.datastax.com/documentation/articles/cassandra/cassandrathenandnow.html),”
     *datastax.com*, September 12, 2013.

13.  “[Introduction to Cassandra Query Language](http://www.datastax.com/documentation/cql/3.1/cql/cql_intro_c.html),” DataStax, Inc., 2014.

14.  Samuel Axon:     “[3% of Twitter's Servers Dedicated to Justin Bieber](http://mashable.com/2010/09/07/justin-bieber-twitter/),” *mashable.com*, September 7, 2010.

15.  “[Riak 1.4.8 Docs](http://docs.basho.com/riak/1.4.8/),” Basho Technologies, Inc., 2014.

16.  Richard Low:     “[The Sweet Spot for Cassandra Secondary Indexing](http://www.wentnet.com/blog/?p=77),” *wentnet.com*, October 21, 2013.

17.  Zachary Tong: “[Customizing Your Document Routing](http://www.elasticsearch.org/blog/customizing-your-document-routing/),” *elasticsearch.org*, June 3, 2013.

18.  “[Apache Solr Reference Guide](https://cwiki.apache.org/confluence/display/solr/Apache+Solr+Reference+Guide),” Apache Software Foundation, 2014.

19.  Andrew Pavlo:     “[H-Store Frequently Asked Questions](http://hstore.cs.brown.edu/documentation/faq/),” *hstore.cs.brown.edu*, October 2013.

20.  “[Amazon DynamoDB Developer Guide](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/),” Amazon Web Services, Inc., 2014.

21.  Rusty Klophaus:     “[Difference Between 2I and Search](http://lists.basho.com/pipermail/riak-users_lists.basho.com/2011-October/006220.html),” email to *riak-users* mailing list, *lists.basho.com*, October 25, 2011.

22.  Donald K. Burleson:     “[Object Partitioning in Oracle](http://www.dba-oracle.com/art_partit.htm),”*dba-oracle.com*, November 8, 2000.

23.  Eric Evans:     “[Rethinking Topology in Cassandra](http://www.slideshare.net/jericevans/virtual-nodes-rethinking-topology-in-cassandra),” at *ApacheCon Europe*, November 2012.

24.  Rafał Kuć:     “[Reroute API Explained](http://elasticsearchserverbook.com/reroute-api-explained/),”     *elasticsearchserverbook.com*, September 30, 2013.

25.  “[Project Voldemort Documentation](http://www.project-voldemort.com/voldemort/),” *project-voldemort.com*.

26.  Enis Soztutar:     “[Apache HBase Region Splitting and Merging](http://hortonworks.com/blog/apache-hbase-region-splitting-and-merging/),” *hortonworks.com*, February 1, 2013.

27.  Brandon Williams:     “[Virtual Nodes in Cassandra 1.2](http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2),” *datastax.com*, December 4, 2012.

28.  Richard Jones:     “[libketama: Consistent Hashing Library for Memcached Clients](https://www.metabrew.com/article/libketama-consistent-hashing-algo-memcached-clients),” *metabrew.com*, April 10, 2007.

29.  Branimir Lambov:     “[New Token Allocation Algorithm in Cassandra 3.0](http://www.datastax.com/dev/blog/token-allocation-algorithm),” *datastax.com*, January 28, 2016.

30.  Jason Wilder:     “[Open-Source Service Discovery](http://jasonwilder.com/blog/2014/02/04/service-discovery-in-the-cloud/),” *jasonwilder.com*, February 2014.

31.  Kishore Gopalakrishna, Shi Lu, Zhen Zhang, et al.:     “[Untangling Cluster Management with Helix](http://www.socc2012.org/helix_onecol.pdf?attredirects=0),” at *ACM Symposium on Cloud Computing* (SoCC), October 2012.
     [doi:10.1145/2391229.2391248](http://dx.doi.org/10.1145/2391229.2391248)

32.  “[Moxi 1.8 Manual](http://docs.couchbase.com/moxi-manual-1.8/),” Couchbase, Inc., 2014.

33.  Shivnath Babu and Herodotos Herodotou:     “[Massively Parallel Databases and MapReduce Systems](http://research.microsoft.com/pubs/206464/db-mr-survey-final.pdf),” *Foundations and Trends in Databases*,     volume 5, number 1, pages 1–104, November 2013.[doi:10.1561/1900000036](http://dx.doi.org/10.1561/1900000036)

------

|         上一章         |              目錄               |         下一章         |
| :--------------------: | :-----------------------------: | :--------------------: |
| [第五章：複製](ch5.md) | [設計資料密集型應用](README.md) | [第七章：事務](ch7.md) |

ou



